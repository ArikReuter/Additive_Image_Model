{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B62S6bRZHK0S"
      },
      "source": [
        "# Preliminary Results Ablation Study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJQlJwakQ4Uo"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLjBb6ziWc75"
      },
      "outputs": [],
      "source": [
        "result_path = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vox-Il1DuHTu",
        "outputId": "2b5ea9f2-1e82-459d-9e69-d5085318d01e"
      },
      "outputs": [],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvoctCy0uTFp",
        "outputId": "fe7a626f-abc8-4321-a77d-3eeb357292db"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning==1.4.5\n",
        "!pip install  torchtext==0.6.0\n",
        "!pip install lmdb\n",
        "!pip install pytorch-fid\n",
        "!pip install lpips\n",
        "!pip install torchmetrics==0.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_aWn7QlPQ7x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiMab-jnuoHT"
      },
      "outputs": [],
      "source": [
        "from templates_latent import celeba64d2c_autoenc_latent\n",
        "from templates import celeba64d2c_autoenc, train\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pcFhYpKuUmB"
      },
      "outputs": [],
      "source": [
        "gpus = [0]\n",
        "conf = celeba64d2c_autoenc_latent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPGL_6jgu0gC"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVMvfFo9TJ8V"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"\"\n",
        "MODEL_PATH = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87TW-Nwc2DVX"
      },
      "outputs": [],
      "source": [
        "LATENTS_PATH = f\"{MODEL_PATH}/latent.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRCxgy-NafUl"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDY7b2iRTJ8X"
      },
      "outputs": [],
      "source": [
        "images = torch.load(f\"{DATA_PATH}images.pt\")  #load images for interpolation and as basis for manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmuGv1mVmYgu",
        "outputId": "9b1510d9-999e-4b38-f424-0948b4ae644b"
      },
      "outputs": [],
      "source": [
        "print(f\"number of images: {len(images)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqi0SqIORqa-"
      },
      "outputs": [],
      "source": [
        "with open(LATENTS_PATH, \"rb\") as f:  #load the latent representations of all the images by the autoencoder\n",
        "  latents = torch.load(f)\n",
        "latents = latents['conds'].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VScbshmJSUKA"
      },
      "outputs": [],
      "source": [
        "label_pair_lis = []\n",
        "for i in range(3):\n",
        "  for j in range(i+1, 3):\n",
        "    label_pair_lis.append((i,j))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C15_XRTTD9r"
      },
      "outputs": [],
      "source": [
        "# load results for normal vectors and models to make predictions for quadrants\n",
        "normal_vec_dict = {}\n",
        "\n",
        "for quadrant_pair in label_pair_lis:\n",
        "  normal_vec_dict[quadrant_pair] = np.load(f\"{MODEL_PATH}/normal_vector_{quadrant_pair}.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RX2DB8ETJ8Y"
      },
      "outputs": [],
      "source": [
        "gpus = [0]\n",
        "conf = celeba64d2c_autoenc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbjtSHD9hLva"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SimpleDataset(Dataset):  #define dataset to match format of DAEs\n",
        "  def __init__(self, images):\n",
        "    self.images = images\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img = self.images[idx]\n",
        "    return {'img': img, 'index': idx}\n",
        "\n",
        "dataset_images = SimpleDataset(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGd5Di-Yus6w"
      },
      "outputs": [],
      "source": [
        "# Define correct config for DAEs\n",
        "\n",
        "conf.data_name = 'ffhqlmdb256'\n",
        "conf.make_dataset = lambda: dataset_images\n",
        "conf.warmup = 0\n",
        "conf.total_samples = len(images)*100\n",
        "conf.net_ch_mult = (1, 2, 4, 8)\n",
        "conf.net_enc_channel_mult = (1, 2, 4, 8, 8)\n",
        "conf.eval_every_samples = len(images)*10\n",
        "conf.eval_ema_every_samples = len(images)*10\n",
        "conf.eval_num_images =  BATCH_SIZE\n",
        "conf.batch_size = BATCH_SIZE\n",
        "conf.batch_size_eval = BATCH_SIZE\n",
        "conf.sample_size = BATCH_SIZE\n",
        "conf.sample_every_samples = len(images)*10\n",
        "conf.continue_from = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioNeqBdyvQ1h",
        "outputId": "3a008e7a-97b9-4fbe-c4f6-e3b95433dc4e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.seed:Global seed set to 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model params: 86.63 M\n"
          ]
        }
      ],
      "source": [
        "# load trained DAE\n",
        "\n",
        "from templates import LitModel\n",
        "model = LitModel(conf)\n",
        "state = torch.load(f'{MODEL_PATH}/last.ckpt', map_location='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyYBABePvgni"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(state['state_dict'], strict=False)\n",
        "model.ema_model.eval()\n",
        "model.ema_model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr1nvYm-dzC1"
      },
      "source": [
        "## Create Synthetic tabular data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOvQOuOyd6QV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_synthetic_table_normal_response(function_list, noise_variance=1, n_datapoints=len(images), x_min=0, x_max=1, equally_spaced=True):\n",
        "    \"\"\"\n",
        "    Create synthetic tabular data with the functions given\n",
        "    such that the response y is given as y = f_1(x_1) + f_2(x_2) + ... + f_k(x_k) + eps for eps ~ N(0, noise_variance)\n",
        "    It plots individual graphs for each function in a single row.\n",
        "\n",
        "    Params:\n",
        "        function_list: List of functions [f_1, f_2, ..., f_k] to use for the response\n",
        "        noise_variance: The variance of the added noise\n",
        "        n_datapoints: Number of samples to generate\n",
        "        x_min: Minimum x_value for each covariate\n",
        "        x_max: Maximum x_value for each covariate\n",
        "        equally_spaced: If True, use equally spaced covariates; otherwise, use random uniform covariates\n",
        "\n",
        "    Returns:\n",
        "        X: Matrix containing the training data, has shape n_datapoints x len(function_list)\n",
        "        y: Response variable, generated as specified\n",
        "    \"\"\"\n",
        "    # Generate covariates (X) based on the specified method\n",
        "    if equally_spaced:\n",
        "        X = np.repeat(np.linspace(x_min, x_max, n_datapoints).reshape(-1, 1), len(function_list), axis = 1)\n",
        "    else:\n",
        "        X = np.random.uniform(x_min, x_max, size=(n_datapoints, len(function_list)))\n",
        "\n",
        "    # Initialize an array to store the response variable\n",
        "    num_functions = len(function_list)\n",
        "    Y = np.zeros((num_functions, n_datapoints))\n",
        "\n",
        "\n",
        "    # Create subplots for each function\n",
        "\n",
        "\n",
        "\n",
        "    # Generate response variable y based on the given functions and noise\n",
        "    for i, func in enumerate(function_list):\n",
        "        Y[i] = func(X[:, i])\n",
        "\n",
        "    Y = Y - np.mean(Y, axis = 1).reshape(-1, 1)\n",
        "\n",
        "    noise = np.random.normal(0, np.sqrt(noise_variance), size=n_datapoints)\n",
        "    if num_functions > 1:\n",
        "      fig, axs = plt.subplots(1, num_functions, figsize=(15, 5))\n",
        "      for i, func in enumerate(function_list):\n",
        "          axs[i].scatter(X[:, i], Y[i] + noise, s=5, label=f'f_{i+1} with noise')\n",
        "          axs[i].scatter(X[:, i], Y[i], s=5, label=f'f_{i+1}', c = \"red\")\n",
        "          axs[i].set_xlabel(f'x_{i+1}')\n",
        "          axs[i].set_ylabel('y')\n",
        "          axs[i].legend()\n",
        "\n",
        "    else:\n",
        "      fig, axs = plt.subplots(1, num_functions, figsize=(5, 5))\n",
        "      axs.scatter(X[:, 0], Y[0] + noise, s=5, label=f'f_{1}')\n",
        "      axs.set_xlabel(f'x_{1}')\n",
        "      axs.set_ylabel('y')\n",
        "      axs.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Add Gaussian noise with the specified variance\n",
        "    y_mean = np.sum(Y, axis = 0)\n",
        "\n",
        "\n",
        "\n",
        "    y = y_mean + noise\n",
        "\n",
        "\n",
        "    #fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
        "\n",
        "    #axs[0].scatter(np.mean(X, axis = 1), y_mean, s=5, label=f'final result without noise')\n",
        "\n",
        "    #axs[1].scatter(np.mean(X, axis = 1), y, s=5, label=f'final result with noise')\n",
        "    #axs[1].scatter(np.mean(X, axis = 1), y_mean, s=1, c = \"red\")\n",
        "\n",
        "    return X, y, Y, noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXc_um_cd7I1"
      },
      "outputs": [],
      "source": [
        "f1 = lambda x: np.sin(2*np.pi*x)\n",
        "f2 = lambda x: 2*x\n",
        "f3 = lambda x: x**2\n",
        "\n",
        "DATA_FUNCTIONS = [f1, f2, f3]\n",
        "N_FEATURES_TABULAR = len(DATA_FUNCTIONS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "hfZzObzhioBw",
        "outputId": "4b5177ad-49c4-46f5-f8db-80c75da661dd"
      },
      "outputs": [],
      "source": [
        "X_tabular, y_response, Y_response, y_noise = create_synthetic_table_normal_response(DATA_FUNCTIONS, noise_variance = 1e-2, equally_spaced = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UzeLXM9B8o-"
      },
      "source": [
        "## Add effect of images to synthetic response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGXe-gcMCAmu"
      },
      "source": [
        "define the effect of the image as follows:\n",
        "Use an effect of the R-value of the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBWd5bLussyD"
      },
      "outputs": [],
      "source": [
        "def generate_img(r, g=0.5, b=0.5, size=64):\n",
        "    \"\"\"\n",
        "    Generate an image of shape (3, size, size) where all pixels are colored according to r, g, b.\n",
        "\n",
        "    Args:\n",
        "        r (float): Red channel intensity (between 0 and 1).\n",
        "        g (float): Green channel intensity (between 0 and 1).\n",
        "        b (float): Blue channel intensity (between 0 and 1).\n",
        "        size (int): Size of the image (width and height).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Generated image tensor with shape (3, size, size).\n",
        "    \"\"\"\n",
        "    if not (0 <= r <= 1) or not (0 <= g <= 1) or not (0 <= b <= 1):\n",
        "        raise ValueError(\"RGB values should be between 0 and 1\")\n",
        "\n",
        "    # Create an empty image tensor\n",
        "    image = torch.zeros(3, size, size)\n",
        "\n",
        "    # Fill the image tensor with the specified RGB values\n",
        "    image[0, :, :] = r\n",
        "    image[1, :, :] = g\n",
        "    image[2, :, :] = b\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def interpolate_images_linear_torch(img1, img2, n_images=10):\n",
        "    # Ensure the input images are PyTorch tensors\n",
        "    if not (isinstance(img1, torch.Tensor) and isinstance(img2, torch.Tensor)):\n",
        "        raise TypeError(\"img1 and img2 must be PyTorch tensors\")\n",
        "\n",
        "    # Calculate the average RGB values for each image\n",
        "    r1, g1, b1 = img1.mean(dim=[1, 2])\n",
        "    r2, g2, b2 = img2.mean(dim=[1, 2])\n",
        "\n",
        "    # Create a sequence of interpolated RGB values\n",
        "    r_values = torch.linspace(r1, r2, steps=n_images)\n",
        "    g_values = torch.linspace(g1, g2, steps=n_images)\n",
        "    b_values = torch.linspace(b1, b2, steps=n_images)\n",
        "\n",
        "    # Generate and store the interpolated images\n",
        "    interpolated_images = []\n",
        "    for r, g, b in zip(r_values, g_values, b_values):\n",
        "        # Create an image with the interpolated color\n",
        "        img = torch.full_like(img1, 0)  # Initialize with zeros\n",
        "        img[0, :, :] = r  # Red channel\n",
        "        img[1, :, :] = g  # Green channel\n",
        "        img[2, :, :] = b  # Blue channel\n",
        "        interpolated_images.append(img)\n",
        "\n",
        "    return torch.stack(interpolated_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAd5u81NkowS"
      },
      "outputs": [],
      "source": [
        "def average_red_value_from_tensor(image_tensor):\n",
        "    # Extract the Red channel (channel 0)\n",
        "    red_channel = image_tensor[0, :, :]\n",
        "\n",
        "    # Calculate the average Red value\n",
        "    average_red = torch.mean(red_channel)\n",
        "\n",
        "    return average_red.item()\n",
        "\n",
        "def average_rgb_vals_from_tensor(image_tensor):\n",
        "    \"\"\"\n",
        "    Calculate the average RGB values from a given image tensor.\n",
        "\n",
        "    Args:\n",
        "        image_tensor (torch.Tensor): Input image tensor with shape (3, height, width).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the average values for Red, Green, and Blue channels.\n",
        "    \"\"\"\n",
        "    if image_tensor.shape[0] != 3:\n",
        "        raise ValueError(\"Input tensor should have shape (3, height, width) for RGB image\")\n",
        "\n",
        "    # Calculate the average values for each RGB channel\n",
        "    average_red = torch.mean(image_tensor[0, :, :])\n",
        "    average_green = torch.mean(image_tensor[1, :, :])\n",
        "    average_blue = torch.mean(image_tensor[2, :, :])\n",
        "\n",
        "    return (average_red.item(), average_green.item(), average_blue.item())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def average_red_value_batch(image_batch):\n",
        "    # Extract the Red channels (channel 0) for all images in the batch\n",
        "    red_channels = image_batch[:, 0, :, :]\n",
        "\n",
        "    # Calculate the average Red value for each image in the batch\n",
        "    average_red_values = torch.mean(red_channels, dim=(1, 2))\n",
        "\n",
        "    return average_red_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR386IFfCAV8"
      },
      "outputs": [],
      "source": [
        "r_vals = average_red_value_batch(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHG4YgWbCAcz"
      },
      "outputs": [],
      "source": [
        "# use a linear function of how much the image x-position varies between the upper left hand-side corner and the upper right-hand-side corner\n",
        "\n",
        "min_xvals = torch.min(r_vals)\n",
        "max_xvals = torch.max(r_vals)\n",
        "\n",
        "\n",
        "standardize_x_vals = lambda x: ((x-min_xvals) / (max_xvals - min_xvals))   #scale position values between 0 and 1\n",
        "\n",
        "\n",
        "f_img_raw = lambda x: torch.sin(2*torch.pi*x)\n",
        "f_img = lambda x: f_img_raw(standardize_x_vals(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldAMiefjAZKs"
      },
      "outputs": [],
      "source": [
        "def f_img_raw(x):\n",
        "  try:\n",
        "    return np.sin(2*np.pi*x)\n",
        "  except:\n",
        "    return torch.sin(2*torch.pi*x)\n",
        "\n",
        "f_img = lambda x: f_img_raw(standardize_x_vals(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "FBSqyCRTxnx_",
        "outputId": "99eaad0a-6485-4fd4-962b-d37211e7a66a"
      },
      "outputs": [],
      "source": [
        "min_img_x = standardize_x_vals(r_vals).min().numpy()\n",
        "max_img_x = standardize_x_vals(r_vals).max().numpy()\n",
        "x_test_img = np.linspace(min_img_x, max_img_x, 1000)\n",
        "\n",
        "y_test_img = f_img_raw(torch.tensor(x_test_img)).numpy()\n",
        "y_test_img = y_test_img - np.mean(y_test_img)\n",
        "\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "figure(figsize=(30, 10.5), dpi=80)\n",
        "\n",
        "plt.scatter(x_test_img, y_test_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyTuO1dbCAc0"
      },
      "outputs": [],
      "source": [
        "img_effect = f_img(r_vals).numpy()\n",
        "img_effect = (img_effect - np.mean(img_effect))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CgPsJOx4VFP",
        "outputId": "cecce45a-335c-42d1-94d7-3d0122ddc295"
      },
      "outputs": [],
      "source": [
        "np.mean(img_effect), np.std(img_effect),  np.max(img_effect), np.min(img_effect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMVIqPcDYqqj"
      },
      "outputs": [],
      "source": [
        "y_response = y_response + img_effect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXkScq7JF1gN"
      },
      "outputs": [],
      "source": [
        "y_response = y_response - np.mean(y_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGsjOe4FGpzO",
        "outputId": "2833bffc-66e5-4c3d-e2d5-9e974be016d5"
      },
      "outputs": [],
      "source": [
        "np.mean(y_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs-wuRAcuxTE"
      },
      "source": [
        "## Define NAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVucKgiwu2Kt"
      },
      "outputs": [],
      "source": [
        "class FeatureNN(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 shallow_units: int,   # number of neurons in first layer\n",
        "                 hidden_units = [],  # tuple of numbers of hidden units\n",
        "                 activation = torch.nn.ReLU(),\n",
        "                 dropout: float = .5,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define Layers\n",
        "        self.layers = torch.nn.ModuleList([\n",
        "            torch.nn.Linear(shallow_units if i == 0 else hidden_units[i - 1], hidden_units[i])\n",
        "            for i in range(len(hidden_units))\n",
        "        ])\n",
        "\n",
        "        self.layers.insert(0, torch.nn.Linear(1, shallow_units))\n",
        "        self.output_layer = torch.nn.Linear(hidden_units[-1], 1)\n",
        "\n",
        "        # Dropout and activation\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "        self.activation = activation\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            x = self.activation(x)\n",
        "            x = self.dropout(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class NAM(torch.nn.Module):\n",
        "  def __init__(self,\n",
        "                n_features,\n",
        "                shallow_units: int,   # number of neurons in first layer\n",
        "                hidden_units = [],  # tuple of numbers of hidden units\n",
        "                activation = torch.nn.ReLU(),\n",
        "                dropout: float = .5,\n",
        "                feature_dropout = 0.0,\n",
        "                return_output_lis = False\n",
        "                ):\n",
        "      super().__init__()\n",
        "\n",
        "      self.shallow_units = shallow_units\n",
        "      self.hidden_units = hidden_units\n",
        "      self.activation = activation\n",
        "      self.dropout = dropout\n",
        "\n",
        "      self.n_features = n_features\n",
        "      self.feature_dropout_rate = feature_dropout\n",
        "      self.return_output_lis = return_output_lis\n",
        "\n",
        "      self.feature_nns = torch.nn.ModuleList([\n",
        "            FeatureNN(shallow_units=shallow_units,\n",
        "                      hidden_units=hidden_units,\n",
        "                      activation=activation,\n",
        "                      dropout=dropout)\n",
        "            for i in range(n_features)\n",
        "        ])\n",
        "\n",
        "      self.bias = torch.nn.Parameter(torch.zeros(1))\n",
        "      self.feature_dropout = torch.nn.Dropout(p=self.feature_dropout_rate)\n",
        "\n",
        "  def forward(self, x, f):\n",
        "    eta = self.bias\n",
        "    output_lis = []\n",
        "    for feature, mod in zip(f.T, self.feature_nns):\n",
        "      feature = feature.unsqueeze(-1)\n",
        "      ri = mod(feature)\n",
        "      output_lis.append(ri)\n",
        "\n",
        "    if self.return_output_lis:\n",
        "      return output_lis\n",
        "\n",
        "    else:\n",
        "       conc_out = torch.cat(output_lis, dim=-1)\n",
        "       dropout_out = self.feature_dropout(conc_out)\n",
        "       out = torch.sum(dropout_out, dim=-1) + self.bias\n",
        "\n",
        "       return out\n",
        "\n",
        "\n",
        "class NAM_plus_CNN(torch.nn.Module):\n",
        "  def __init__(self,\n",
        "                pretrained_encoder,\n",
        "                mlp,\n",
        "                feat_nam,\n",
        "                ):\n",
        "      super().__init__()\n",
        "\n",
        "      self.pretrained_encoder = pretrained_encoder\n",
        "      self.mlp = mlp\n",
        "      self.feat_nam = feat_nam\n",
        "      self.feat_nam.return_output_lis = True\n",
        "\n",
        "  def forward(self, image, features):\n",
        "    output_lis_nam = self.feat_nam(_, features)\n",
        "\n",
        "    res_cnn = self.pretrained_encoder.encode(image)\n",
        "    res_cnn = self.mlp(res_cnn)\n",
        "\n",
        "    output_lis = output_lis_nam + [res_cnn]\n",
        "    output_lis = torch.cat(output_lis, dim=-1)\n",
        "\n",
        "\n",
        "    output_lis = self.feat_nam.feature_dropout(output_lis)\n",
        "    out = torch.sum(output_lis, dim=-1) + self.feat_nam.bias\n",
        "    return out\n",
        "\n",
        "class NAM_plus_MLP(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  class for a NAM that also includes the effect of a an embedding vector\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "                mlp,\n",
        "                feat_nam,\n",
        "                ):\n",
        "      super().__init__()\n",
        "\n",
        "      self.mlp = mlp\n",
        "      self.feat_nam = feat_nam\n",
        "      self.feat_nam.return_output_lis = True\n",
        "\n",
        "  def forward(self, features, latent_vec):\n",
        "    output_lis_nam = self.feat_nam(_, features)\n",
        "\n",
        "    res_cnn = self.mlp(latent_vec)\n",
        "\n",
        "    output_lis = output_lis_nam + [res_cnn]\n",
        "    output_lis = torch.cat(output_lis, dim=-1)\n",
        "\n",
        "\n",
        "    output_lis = self.feat_nam.feature_dropout(output_lis)\n",
        "    out = torch.sum(output_lis, dim=-1) + self.feat_nam.bias\n",
        "    return out\n",
        "\n",
        "class One_Layer_MLP(torch.nn.Module):\n",
        "  def __init__(self,  n_input_units):\n",
        "\n",
        "    super(One_Layer_MLP, self).__init__()\n",
        "\n",
        "    self.n_input_units = n_input_units\n",
        "    self.fc1 = torch.nn.Linear(n_input_units, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Linear_skip_block(nn.Module):\n",
        "  \"\"\"\n",
        "  Block of linear layer + softplus + skip connection +  dropout  + batchnorm\n",
        "  \"\"\"\n",
        "  def __init__(self, n_input, dropout_rate):\n",
        "    super(Linear_skip_block, self).__init__()\n",
        "\n",
        "    self.fc = nn.Linear(n_input, n_input)\n",
        "    self.act = torch.nn.LeakyReLU()\n",
        "\n",
        "    self.bn = nn.BatchNorm1d(n_input, affine = True)\n",
        "    self.drop = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x0 = x\n",
        "    x = self.fc(x)\n",
        "    x = self.act(x)\n",
        "    x = x0 + x\n",
        "    x = self.drop(x)\n",
        "    x = self.bn(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Linear_block(nn.Module):\n",
        "  \"\"\"\n",
        "  Block of linear layer dropout  + batchnorm\n",
        "  \"\"\"\n",
        "  def __init__(self, n_input, n_output, dropout_rate):\n",
        "    super(Linear_block, self).__init__()\n",
        "\n",
        "    self.fc = nn.Linear(n_input, n_output)\n",
        "    self.act = torch.nn.LeakyReLU()\n",
        "    self.bn = nn.BatchNorm1d(n_output, affine = True)\n",
        "    self.drop = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc(x)\n",
        "    x = self.act(x)\n",
        "    x = self.drop(x)\n",
        "    x = self.bn(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, n_input_units, n_hidden_units, n_skip_layers, dropout_rate):\n",
        "\n",
        "    super(MLP, self).__init__()\n",
        "    self.n_input_units = n_input_units\n",
        "    self.n_hidden_units = n_hidden_units\n",
        "    self.n_skip_layers = n_skip_layers\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "    self.linear1 = Linear_block(n_input_units, n_hidden_units, dropout_rate)    # initial linear layer\n",
        "    self.hidden_layers = torch.nn.Sequential(*[Linear_skip_block(n_hidden_units, dropout_rate) for _ in range(n_skip_layers)])  #hidden skip-layers\n",
        "\n",
        "    self.linear_final =  torch.nn.Linear(n_hidden_units, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.hidden_layers(x)\n",
        "    x = self.linear_final(x)\n",
        "\n",
        "    return(x)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self, pretrained_model, mlp):\n",
        "    super(CNN, self).__init__()\n",
        "\n",
        "    self.pretrained_model = pretrained_model\n",
        "    self.pretrained_model.requires_grad = False\n",
        "    self.mlp = mlp\n",
        "\n",
        "  def forward(self, x, f):\n",
        "    with torch.no_grad():\n",
        "      x = self.pretrained_model.encode(x)\n",
        "    x = self.mlp(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yygEMGkZve0G"
      },
      "outputs": [],
      "source": [
        "def var_exp_score(predictions, targets):\n",
        "  mean_sum_of_squares = torch.mean((predictions - targets)**2)\n",
        "  variance_targets = torch.var(targets)\n",
        "  var_exp = 1- mean_sum_of_squares/variance_targets\n",
        "\n",
        "  return var_exp\n",
        "\n",
        "def coef_det(x, y):\n",
        "  sum_x = torch.sum(x)\n",
        "  sum_y = torch.sum(y)\n",
        "  n = len(x)\n",
        "\n",
        "  numerator = n * torch.sum(x * y) - sum_x*sum_x\n",
        "  denominator = (n * torch.sum(x**2) - sum_x**2)**0.5 * (n * torch.sum(y**2) - sum_y**2)**0.5\n",
        "\n",
        "  return numerator/denominator\n",
        "\n",
        "\n",
        "def mad_explained(predictions, targets):\n",
        "  mean_sum_of_ad = torch.mean(torch.abs(predictions - targets))\n",
        "  deviation_median = torch.mean(torch.abs(targets - torch.median(targets)))\n",
        "\n",
        "  mad_exp = 1 - mean_sum_of_ad/deviation_median\n",
        "\n",
        "  return mad_exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezfGiqE3zRdJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "#Validation function\n",
        "\n",
        "def validate(model, dataloader, loss_fun):\n",
        "    val_loss_lis = []\n",
        "\n",
        "    target_lis = []\n",
        "    pred_lis = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "\n",
        "          x, f, y = batch\n",
        "          x = x.to(device)\n",
        "          f = f.to(device)\n",
        "          y = y.to(device)\n",
        "          pred = model(x, f)\n",
        "          pred = pred.squeeze(-1)\n",
        "          loss = loss_fun(pred, y)\n",
        "          val_loss_lis.append(loss.cpu().detach())\n",
        "\n",
        "          target_lis.append(y.detach().cpu())\n",
        "          pred_lis.append(pred.detach().cpu())\n",
        "\n",
        "    mean_loss = np.mean(np.array(val_loss_lis))\n",
        "    median_loss = np.median(np.array(val_loss_lis))\n",
        "\n",
        "    target_ten, pred_ten = torch.cat(target_lis), torch.cat(pred_lis)\n",
        "    var_exp = var_exp_score(pred_ten, target_ten)\n",
        "    mad_exp = mad_explained(pred_ten, target_ten)\n",
        "    r_score = coef_det(pred_ten, target_ten)\n",
        "    return mean_loss, median_loss, var_exp, mad_exp, r_score\n",
        "\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_loop(model, optimizer, loss_fun, trainset, valset, print_mod, device, n_epochs, save_path = None, early_stopping = True, n_epochs_early_stopping = 5):\n",
        "    \"\"\"\n",
        "    train the model\n",
        "    Args:\n",
        "        model: The model to train\n",
        "        optimizer: The used optimizer\n",
        "        loss_fun: The used loss function\n",
        "        trainset: The dataset to train on\n",
        "        valset: The dataset to use for validation\n",
        "        print_mod: Number of epochs to print result after\n",
        "        device: Either \"cpu\" or \"cuda\"\n",
        "        n_epochs: Number of epochs to train\n",
        "        save_path: Path to save the model's state dict\n",
        "        config: config file from the model to train\n",
        "        sparse_ten (bool): if a sparse tensor is used for each batch\n",
        "    \"\"\"\n",
        "    if early_stopping == True:\n",
        "      n_early_stopping = n_epochs_early_stopping\n",
        "      past_val_losses = []\n",
        "\n",
        "    loss_lis = []\n",
        "    target_lis = []\n",
        "    pred_lis = []\n",
        "\n",
        "    loss_lis_all = []\n",
        "    val_loss_lis_all = []\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(n_epochs):\n",
        "      start = time.time()\n",
        "      for iter, batch in enumerate(tqdm(trainset)):\n",
        "\n",
        "        x, f, y = batch\n",
        "        x = x.to(device)\n",
        "        f = f.to(device)\n",
        "        y = y.to(device)\n",
        "        pred = model(x, f)\n",
        "        pred = pred.squeeze(-1)\n",
        "\n",
        "\n",
        "        loss = loss_fun(pred, y)\n",
        "        #print(loss)\n",
        "\n",
        "        optimizer.zero_grad()       # clear previous gradients\n",
        "        loss.backward()             # backprop\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_lis.append(loss.cpu().detach())\n",
        "        target_lis.append(y.detach().cpu())\n",
        "        pred_lis.append(pred.detach().cpu())\n",
        "\n",
        "      if epoch % print_mod == 0:\n",
        "\n",
        "        end = time.time()\n",
        "        time_delta = end - start\n",
        "\n",
        "        mean_loss = np.mean(np.array(loss_lis))\n",
        "        median_loss = np.median(np.array(loss_lis))\n",
        "\n",
        "        target_ten, pred_ten = torch.cat(target_lis), torch.cat(pred_lis)\n",
        "        var_exp = var_exp_score(pred_ten, target_ten)\n",
        "        mad_exp = mad_explained(pred_ten, target_ten)\n",
        "        r_score = coef_det(pred_ten, target_ten)\n",
        "\n",
        "        target_lis = []\n",
        "        pred_lis = []\n",
        "\n",
        "\n",
        "\n",
        "        loss_lis_all += loss_lis\n",
        "\n",
        "        loss_lis = []\n",
        "\n",
        "\n",
        "\n",
        "        mean_loss_val, median_loss_val, var_exp_val, mad_exp_val, val_r_score = validate(model, val_loader, loss_fun = loss_fun)\n",
        "\n",
        "        val_loss_lis_all.append(mean_loss_val)\n",
        "\n",
        "\n",
        "\n",
        "        print(f'Epoch nr {epoch}: mean_train_loss = {mean_loss}, median_train_loss = {median_loss}, train_var_exp = {var_exp}, train_mad_exp = {mad_exp}, train_r = {r_score}, elapsed time: {time_delta}')\n",
        "        print(f'Epoch nr {epoch}: mean_valid_loss = {mean_loss_val}, median_valid_loss = {median_loss_val}, valid_var_exp = {var_exp_val}, valid_mad_exp = {mad_exp_val},  valid_r = {val_r_score}')\n",
        "\n",
        "\n",
        "\n",
        "        # early stopping based on median validation loss:\n",
        "        if early_stopping:\n",
        "          if len(past_val_losses) == 0 or mean_loss_val < min(past_val_losses):\n",
        "            print(\"save model\")\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "\n",
        "          if len(past_val_losses) >= n_early_stopping:\n",
        "            if mean_loss_val > max(past_val_losses):\n",
        "              print(f\"Early stopping because the median validation loss has not decreased since the last {n_early_stopping} epochs\")\n",
        "              return loss_lis_all, val_loss_lis_all\n",
        "            else:\n",
        "              past_val_losses = past_val_losses[1:] + [mean_loss_val]\n",
        "          else:\n",
        "            past_val_losses = past_val_losses + [mean_loss_val]\n",
        "\n",
        "\n",
        "\n",
        "    return loss_lis_all, val_loss_lis_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS2QFNXgBsS1"
      },
      "source": [
        "### Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4hobIMXaGVq"
      },
      "outputs": [],
      "source": [
        "class CustomDataset_latents_images(torch.utils.data.Dataset):\n",
        "    \"\"\"Create dataloader with features, latent variables, images and labels\"\"\"\n",
        "    def __init__(self, feature_ten, images_ten, latent_ten, label_ten):\n",
        "\n",
        "        self.images_ten = images_ten\n",
        "        self.feature_ten = feature_ten\n",
        "        self.label_ten = label_ten\n",
        "        self.latent_ten = latent_ten\n",
        "\n",
        "        assert len(feature_ten) == len(label_ten) ==  len(latent_ten) == len(images_ten), \"img_ten and and feature_ten and label ten and latent_ten must have equal size\"\n",
        "    def __len__(self):\n",
        "      return len(self.images_ten)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      img = self.images_ten[idx]\n",
        "      tab_features = self.feature_ten[idx]\n",
        "      latents = self.latent_ten[idx]\n",
        "      y = self.label_ten[idx]\n",
        "\n",
        "      return img, tab_features, latents, y\n",
        "\n",
        "\n",
        "class CustomDataset_latents(torch.utils.data.Dataset):\n",
        "    \"\"\"Create dataloader with features, latent variables and labels\"\"\"\n",
        "    def __init__(self, feature_ten, latent_ten, label_ten):\n",
        "\n",
        "        self.feature_ten = feature_ten\n",
        "        self.label_ten = label_ten\n",
        "        self.latent_ten = latent_ten\n",
        "\n",
        "        assert len(feature_ten) == len(label_ten) ==  len(latent_ten), \"feature_ten and label ten and latent_ten must have equal size\"\n",
        "    def __len__(self):\n",
        "      return len(self.feature_ten)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      tab_features = self.feature_ten[idx]\n",
        "      latents = self.latent_ten[idx]\n",
        "      y = self.label_ten[idx]\n",
        "\n",
        "      return tab_features, latents, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBibCg7CbgZB"
      },
      "outputs": [],
      "source": [
        "def train_test_split_features_images_latents(features, images, latents, targets, train_frac, val_frac, batch_size):\n",
        "\n",
        "    # Create an index vector for shuffling\n",
        "    num_samples = len(features)\n",
        "    index_vector = torch.randperm(num_samples)\n",
        "\n",
        "    # Shuffle all four tensors using the index vector\n",
        "    features = features[index_vector]\n",
        "    images = images[index_vector]\n",
        "    latents = latents[index_vector]\n",
        "    targets = targets[index_vector]\n",
        "\n",
        "    tot_len = len(features)\n",
        "\n",
        "    train_max_idx = int(tot_len*train_frac)\n",
        "    val_max_idx = int(tot_len*val_frac) + train_max_idx\n",
        "\n",
        "    train_features = features[:train_max_idx]\n",
        "    train_images = images[:train_max_idx]\n",
        "    train_latents = latents[:train_max_idx]\n",
        "    train_y = targets[:train_max_idx]\n",
        "\n",
        "    val_features = features[train_max_idx:val_max_idx]\n",
        "    val_images = images[train_max_idx:val_max_idx]\n",
        "    val_latents = latents[train_max_idx:val_max_idx]\n",
        "    val_y = targets[train_max_idx:val_max_idx]\n",
        "\n",
        "    test_features = features[val_max_idx:]\n",
        "    test_images = images[val_max_idx:]\n",
        "    test_latents = latents[val_max_idx:]\n",
        "    test_y = targets[val_max_idx:]\n",
        "\n",
        "    train_set = CustomDataset_latents_images(train_features, train_images, train_latents, train_y)\n",
        "    val_set = CustomDataset_latents_images(val_features, val_images, val_latents, val_y)\n",
        "    test_set = CustomDataset_latents_images(test_features, test_images, test_latents, test_y)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_set, batch_size = batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def train_test_split_features_latents(features, latents, targets, train_frac, val_frac, batch_size):\n",
        "    tot_len = len(features)\n",
        "\n",
        "    # Create an index vector for shuffling\n",
        "    num_samples = len(features)\n",
        "    index_vector = torch.randperm(num_samples)\n",
        "\n",
        "    # Shuffle all three tensors using the index vector\n",
        "    features = features[index_vector]\n",
        "    latents = latents[index_vector]\n",
        "    targets = targets[index_vector]\n",
        "\n",
        "    train_max_idx = int(tot_len*train_frac)\n",
        "    val_max_idx = int(tot_len*val_frac) + train_max_idx\n",
        "\n",
        "    train_features = features[:train_max_idx]\n",
        "    train_latents = latents[:train_max_idx]\n",
        "    train_y = targets[:train_max_idx]\n",
        "\n",
        "    val_features = features[train_max_idx:val_max_idx]\n",
        "    val_latents = latents[train_max_idx:val_max_idx]\n",
        "    val_y = targets[train_max_idx:val_max_idx]\n",
        "\n",
        "    test_features = features[val_max_idx:]\n",
        "    test_latents = latents[val_max_idx:]\n",
        "    test_y = targets[val_max_idx:]\n",
        "\n",
        "    train_set = CustomDataset_latents(train_features, train_latents, train_y)\n",
        "    val_set = CustomDataset_latents(val_features, val_latents, val_y)\n",
        "    test_set = CustomDataset_latents(test_features, test_latents, test_y)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_set, batch_size = batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWRXzLjqeXCs",
        "outputId": "51a6a4db-8b1d-4251-dffa-42edc84af09e"
      },
      "outputs": [],
      "source": [
        "len(images), len(latents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ymAxu4JblAS"
      },
      "outputs": [],
      "source": [
        "train_loader, val_loader, test_loader = train_test_split_features_latents(\n",
        "    features = torch.tensor(X_tabular).float(),\n",
        "    latents = latents,\n",
        "    targets = torch.tensor(y_response).float(),\n",
        "    train_frac = 0.7,\n",
        "    val_frac = 0.2,\n",
        "    batch_size = 512)   # create dataloader with embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYOdTCDHgGId"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO8p8vSxZAv-"
      },
      "source": [
        "#### Train NAM without image effect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWnogfe7ZTqo"
      },
      "outputs": [],
      "source": [
        "pure_nam_save_path = \"\"\n",
        "\n",
        "\n",
        "#model_cnn = CNN(\n",
        "#    pretrained_model = pretrained_encoder,\n",
        "#    mlp = mlp\n",
        "\n",
        "#)\n",
        "\n",
        "\"\"\"\n",
        "nam = NAM(n_features=N_FEATURES_TABULAR,\n",
        "                            shallow_units=20,\n",
        "                            hidden_units=(100, 100, 100),\n",
        "                            activation= torch.nn.ReLU(),\n",
        "                            dropout=0.5,\n",
        "                            feature_dropout = 0.5\n",
        "          )\n",
        "\"\"\"\n",
        "pure_nam = NAM(n_features=N_FEATURES_TABULAR,\n",
        "                            shallow_units=100,\n",
        "                            hidden_units=(100, 100, 100),\n",
        "                            activation= torch.nn.ReLU(),\n",
        "                            dropout=0.2,\n",
        "                            feature_dropout = 0.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6MmF_K0ZUJO"
      },
      "outputs": [],
      "source": [
        "lr = 1e-3\n",
        "opt = torch.optim.AdamW(pure_nam.parameters(), lr = lr, weight_decay=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8R1lW4nZ3gJ",
        "outputId": "76f5a4f2-4726-40e5-9933-d35f7607c6d1"
      },
      "outputs": [],
      "source": [
        "r1 = train_loop(model = pure_nam,\n",
        "           optimizer = opt,\n",
        "           loss_fun = torch.nn.MSELoss(),     #torch.nn.L1Loss4\n",
        "           trainset = train_loader,\n",
        "           valset = val_loader,\n",
        "           print_mod = 1,\n",
        "           device = device,\n",
        "           early_stopping = True,\n",
        "           n_epochs_early_stopping = 3,\n",
        "           save_path = pure_nam_save_path,\n",
        "           n_epochs = 10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHavhBitaCRW",
        "outputId": "18455f2a-7242-4f61-a1df-2978c8deb4b4"
      },
      "outputs": [],
      "source": [
        "pure_nam.load_state_dict(torch.load(pure_nam_save_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3I5FMZqaCRb"
      },
      "outputs": [],
      "source": [
        "pure_nam = pure_nam.eval().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iyufdsFaCRd",
        "outputId": "d1f74bf4-9f13-48d8-9979-d4556641e0c7"
      },
      "outputs": [],
      "source": [
        "validate(pure_nam, test_loader, torch.nn.MSELoss())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOHwXwL6ZD7v"
      },
      "source": [
        "### Train NAM with image effect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOq8ifxRauib"
      },
      "outputs": [],
      "source": [
        "save_path = \"\"\n",
        "\n",
        "mlp = MLP(n_input_units = latents.shape[1],\n",
        "    n_hidden_units = 100,\n",
        "    n_skip_layers =4,\n",
        "    dropout_rate = 0.2)\n",
        "\n",
        "#model_cnn = CNN(\n",
        "#    pretrained_model = pretrained_encoder,\n",
        "#    mlp = mlp\n",
        "\n",
        "#)\n",
        "\n",
        "\"\"\"\n",
        "nam = NAM(n_features=N_FEATURES_TABULAR,\n",
        "                            shallow_units=20,\n",
        "                            hidden_units=(100, 100, 100),\n",
        "                            activation= torch.nn.ReLU(),\n",
        "                            dropout=0.5,\n",
        "                            feature_dropout = 0.5\n",
        "          )\n",
        "\"\"\"\n",
        "nam = NAM(n_features=N_FEATURES_TABULAR,\n",
        "                            shallow_units=100,\n",
        "                            hidden_units=(100, 100, 100),\n",
        "                            activation= torch.nn.ReLU(),\n",
        "                            dropout=0.2,\n",
        "                            feature_dropout = 0.5\n",
        "          )\n",
        "\n",
        "cnam = NAM_plus_MLP(mlp, nam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkHeDR_VgQ6V"
      },
      "outputs": [],
      "source": [
        "lr = 1e-3\n",
        "opt = torch.optim.AdamW(list(cnam.mlp.parameters()) + list(cnam.feat_nam.parameters()), lr = lr, weight_decay=1e-3)\n",
        "#opt = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8AhThdggchs",
        "outputId": "fc6b156d-fa0c-4141-a74a-b855de02590e"
      },
      "outputs": [],
      "source": [
        "\n",
        "r2 = train_loop(model = cnam,\n",
        "           optimizer = opt,\n",
        "           loss_fun = torch.nn.MSELoss(),     #torch.nn.L1Loss4\n",
        "           trainset = train_loader,\n",
        "           valset = val_loader,\n",
        "           print_mod = 1,\n",
        "           device = device,\n",
        "           early_stopping = True,\n",
        "           n_epochs_early_stopping = 5,\n",
        "           save_path = save_path,\n",
        "           n_epochs = 10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5DfTValAW0U",
        "outputId": "0fb0b92c-37d5-48e6-d006-22db787489ab"
      },
      "outputs": [],
      "source": [
        "cnam.load_state_dict(torch.load(save_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V23DNaPmxgZj"
      },
      "outputs": [],
      "source": [
        "cnam = cnam.eval().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w1cfgrQxluw",
        "outputId": "47a20de8-0b67-4030-f2c4-21e47edede11"
      },
      "outputs": [],
      "source": [
        "validate(cnam, test_loader, torch.nn.MSELoss())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLXMVFI-G1gh",
        "outputId": "594c905b-2be6-4628-98d9-3f28d5158c9e"
      },
      "outputs": [],
      "source": [
        "cnam.feat_nam.bias.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNCDVfNUOQTt"
      },
      "outputs": [],
      "source": [
        "unscale_fun = lambda y:  y - torch.mean(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odUltqprn6vv"
      },
      "outputs": [],
      "source": [
        "#path_tabular_effects = f\"{result_path}/Tabular_Effects\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNMZEbOpUuyo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctOzZ3nCStNL"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_feature_nn(feature_nn, x_vals_compute, bias, x_vals_to_plot, ax=None, y_data=None, unscale_fun=None, q = 0, y_gt = None):\n",
        "\n",
        "\n",
        "    with sns.axes_style('whitegrid'):\n",
        "      with sns.color_palette('dark'):\n",
        "\n",
        "        max_fun = lambda x: np.quantile(x, q = 1-q)\n",
        "        min_fun = lambda x: np.quantile(x, q = q)\n",
        "\n",
        "        valid_idx = (x_vals_compute >= min_fun(x_vals_compute)) & (x_vals_compute <= max_fun(x_vals_compute))\n",
        "\n",
        "        x_vals_compute = x_vals_compute[valid_idx]\n",
        "        x_vals_to_plot = x_vals_to_plot[valid_idx]\n",
        "\n",
        "        #if y_data is not None:\n",
        "        #  y_data = y_data[valid_idx]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        min_compute = min(x_vals_compute)\n",
        "        max_compute = max(x_vals_compute)\n",
        "        range_compute = np.linspace(min_compute, max_compute, 10000)\n",
        "\n",
        "        min_plot = min(x_vals_to_plot)\n",
        "        max_plot = max(x_vals_to_plot)\n",
        "        range_plot = np.linspace(min_plot, max_plot, 10000)\n",
        "\n",
        "        feature_nn = feature_nn.eval()\n",
        "        with torch.no_grad():\n",
        "            model_input = torch.tensor(range_compute).to(device).reshape(-1, 1).float()\n",
        "            y_pred = feature_nn(model_input) + bias\n",
        "\n",
        "            if not unscale_fun is None:\n",
        "                y_pred = unscale_fun(y_pred)\n",
        "\n",
        "\n",
        "            if ax is None:\n",
        "                fig, ax = plt.subplots(figsize=(10, 10))\n",
        "            else:\n",
        "                fig = plt.gcf()\n",
        "\n",
        "            fig.set_size_inches(30/2, 10.5/2)\n",
        "            # Plot the model prediction\n",
        "\n",
        "            if y_gt is not None:\n",
        "              sns.lineplot(x = x_vals_to_plot, y = y_gt,  color=\"green\", linewidth=2, label=\"Reference\")\n",
        "\n",
        "            sns.lineplot(x=range_plot, y=y_pred.squeeze().cpu().numpy(), color=\"r\", label=\"Model Prediction\", linewidth=2)\n",
        "\n",
        "\n",
        "            print(x_vals_to_plot.shape)\n",
        "            y_pred_data = feature_nn(torch.tensor(x_vals_to_plot).to(device).float().unsqueeze(1)) + bias\n",
        "\n",
        "            var_exp = var_exp_score(y_pred_data.squeeze().cpu(), torch.tensor(y_gt))\n",
        "            mad_exp = mad_explained(y_pred_data.squeeze().cpu(), torch.tensor(y_gt))\n",
        "            r_score = coef_det(y_pred_data.squeeze().cpu(), torch.tensor(y_gt))\n",
        "\n",
        "            mse = torch.nn.MSELoss()(y_pred_data.squeeze().cpu(), torch.tensor(y_gt))\n",
        "\n",
        "            print( {\n",
        "                \"var_exp\": var_exp.item(),\n",
        "                \"mad_exp\": mad_exp.item(),\n",
        "                \"r_score\": r_score.item(),\n",
        "                \"mse\": mse.item()\n",
        "            })\n",
        "\n",
        "            # Plot the training data\n",
        "            if y_data is not None:\n",
        "                sns.scatterplot(x=x_vals_to_plot, y=y_data, color=\"blue\", alpha=0.5, s = 2, label=\"Test Data\", ax=ax)\n",
        "\n",
        "\n",
        "\n",
        "            # Customize the plot\n",
        "            ax.set_xlabel(\"X\", fontsize=15)\n",
        "            ax.set_ylabel(\"Y\", fontsize=15)\n",
        "            ax.spines[\"top\"].set_visible(False)\n",
        "            ax.spines[\"right\"].set_visible(False)\n",
        "            ax.spines[\"bottom\"].set_linewidth(1.5)\n",
        "            ax.spines[\"left\"].set_linewidth(1.5)\n",
        "            ax.tick_params(axis='both', which='major', labelsize=15, width=1.5)\n",
        "            #ax.set_xticks(fontsize=15)\n",
        "\n",
        "            # Add a title and legend\n",
        "            #ax.set_title(\"Feature Neural Network\", fontsize=20)\n",
        "            ax.legend(fontsize=20, frameon=True,markerscale = 2)\n",
        "\n",
        "            # Add a grid\n",
        "            ax.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            return fig, ax\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "NpNFHSz8MV9r",
        "outputId": "07124c36-fb34-4666-f1bf-b81ebbcbf19b"
      },
      "outputs": [],
      "source": [
        "X_tabular_test, y_response_test, Y_response_test, y_noise_test = create_synthetic_table_normal_response(DATA_FUNCTIONS, noise_variance = 1e-2, n_datapoints = 10000, equally_spaced = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJgaAIsX0lqT",
        "outputId": "616a0e6f-3a66-43a5-d776-1a76134fc678"
      },
      "outputs": [],
      "source": [
        "Y_response.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wExJVs6CUvD7",
        "outputId": "b3822140-2a48-4b97-ba25-1310eec911f4"
      },
      "outputs": [],
      "source": [
        "cnam.feat_nam.bias.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwzaX2oGUvD8"
      },
      "outputs": [],
      "source": [
        "unscale_fun = lambda y:  y - torch.mean(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMbilbQLUvD8"
      },
      "outputs": [],
      "source": [
        "path_tabular_effects = f\"{result_path}/Tabular_Effects\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gTllY3q3oVO1",
        "outputId": "f525eb36-d0c1-40ca-8afd-7fc8759d45b2"
      },
      "outputs": [],
      "source": [
        "for i in range(N_FEATURES_TABULAR):\n",
        "   fig, ax = plot_feature_nn(cnam.feat_nam.feature_nns[i], X_tabular_test[:, i], bias = cnam.feat_nam.bias.item(), x_vals_to_plot = X_tabular_test[:, i], y_data = Y_response_test[i, :] + y_noise_test, q= 0, unscale_fun = unscale_fun, y_gt = Y_response_test[i, :])\n",
        "   name = f\"{path_tabular_effects}/tabular_effect_{i}.png\"\n",
        "   fig.savefig(name, dpi = 400, bbox_inches='tight')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgGpm4kHQ6nO"
      },
      "source": [
        "## Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHiCX4M_H1ke"
      },
      "source": [
        "Interpolate between images $i_1$ and $i_2$ by encoding both with the semantic and stochastic encoder, followed by interpolation between the encodings where simple linear interpolation is used for the semantic encoder and shperical linear interpolation for the stochastic subcode. The images on the \"latent interpolation path\" are then decoded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpC-bVCOe-sX"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDDLRyy_eHIw"
      },
      "outputs": [],
      "source": [
        "def plot_images_and_predictions2(images1, image_preds1, preds_smooth1,\n",
        "                                images2, image_preds2, preds_smooth2, save_path = None, dpi_save = 400,\n",
        "                                ymin=-1.3, ymax=1.3):\n",
        "    \"\"\"\n",
        "    Plot the effect of two sets of images and their predictions.\n",
        "    params:\n",
        "    images1, images2: images to plot\n",
        "    image_preds1, image_preds2: predictions based on images to plot  GROUND TRUTH\n",
        "    preds_smooth1, preds_smooth2: predictions between image_preds to plot PREDICTIONS\n",
        "    \"\"\"\n",
        "    # Assert that the lengths of images and image_preds are equal for both sets\n",
        "    if image_preds1 is not None and images1 is not None:\n",
        "        assert len(images1) == len(image_preds1), \"Length of images1 must be equal to length of image_preds1\"\n",
        "    if image_preds2 is not None and images2 is not None:\n",
        "        assert len(images2) == len(image_preds2), \"Length of images2 must be equal to length of image_preds2\"\n",
        "\n",
        "    with sns.axes_style('darkgrid'):\n",
        "        with sns.color_palette('dark'):\n",
        "            # Adjust figure size to accommodate two sets of images\n",
        "            fig = plt.figure(figsize=(23/2, 10/2), constrained_layout=False, dpi = dpi_save)\n",
        "            # Create a grid with 4 rows: 1 for plot, 3 for images\n",
        "\n",
        "            if images2 is not None:\n",
        "              gs = fig.add_gridspec(3, max(len(images1), len(images2)), height_ratios=[1, 0.5, 0.5], hspace = 0.2, wspace=0.00) #the height ratios are a kinda a hack to cicumvent some weird vertical padding padding\n",
        "            else:\n",
        "              gs = fig.add_gridspec(3, len(images1), height_ratios=[1, 0.5, 0.5], hspace = 0.05, wspace=0.05) #the height ratios are a kinda a hack to cicumvent some weird vertical padding padding\n",
        "\n",
        "            # Plot for predictions\n",
        "            ax3 = fig.add_subplot(gs[0, :])\n",
        "            ax3.set_ylim([ymin, ymax])\n",
        "\n",
        "\n",
        "            if preds_smooth1 is not None:\n",
        "\n",
        "              x_range_plot_smooth_preds1 = np.linspace(0, len(images1) - 1, len(preds_smooth1))\n",
        "              print(len(x_range_plot_smooth_preds1), len(preds_smooth1))\n",
        "              #ax3.scatter(x_range_plot_smooth_preds1, preds_smooth1, c=\"blue\")\n",
        "              ax3.plot(x_range_plot_smooth_preds1, preds_smooth1, '-', markersize=10, c=\"blue\")\n",
        "\n",
        "\n",
        "            if preds_smooth2 is not None:\n",
        "              x_range_plot_smooth_preds2 = np.linspace(0, len(images1) - 1, len(preds_smooth2))\n",
        "              #ax3.scatter(x_range_plot_smooth_preds2, preds_smooth1, c = \"blue\")\n",
        "              print(len(x_range_plot_smooth_preds2), len(preds_smooth2))\n",
        "              ax3.plot(x_range_plot_smooth_preds2, preds_smooth2, '-', markersize=10, c=\"red\")\n",
        "\n",
        "            # Plot first set of predictions\n",
        "            if image_preds1 is not None:\n",
        "                ax3.plot(range(len(image_preds1)), image_preds1, 'o', markersize=5, c=\"blue\", linewidth=3, label = \"Reference\")\n",
        "\n",
        "            # Plot second set of predictions\n",
        "            if image_preds2 is not None:\n",
        "                ax3.plot(range(len(image_preds2)), image_preds2, 'o', markersize=5, c=\"red\", linewidth=3, label = \"Prediction\")\n",
        "\n",
        "\n",
        "\n",
        "            ax3.set_ylabel('Response')\n",
        "            ax3.set_xticks([])  # remove x_ticks\n",
        "            ax3.legend()\n",
        "\n",
        "            # Plot the first set of images\n",
        "\n",
        "            if images1 is not None:\n",
        "              axes1_list = []\n",
        "              for i in range(len(images1)):\n",
        "                  axi = fig.add_subplot(gs[1, i])\n",
        "                  axi.imshow(images1[i].permute(1,2,0).cpu(), aspect = \"equal\")\n",
        "                  axi.xaxis.set_ticklabels([])\n",
        "                  axi.yaxis.set_ticklabels([])\n",
        "                  axi.grid(False)\n",
        "                  axes1_list.append(axi)\n",
        "                  r_value = average_red_value_from_tensor(images1[i])\n",
        "                  #axi.set_xlabel(\"{:.2f}\".format(r_value), fontsize = 10)\n",
        "\n",
        "            # Plot the second set of images\n",
        "            if images2 is not None:\n",
        "              axes2_list = []\n",
        "              for i in range(len(images2)):\n",
        "                  axi = fig.add_subplot(gs[2, i])\n",
        "                  axi.imshow(images2[i].permute(1,2,0).cpu(), aspect = \"equal\")\n",
        "                  axi.xaxis.set_ticklabels([])\n",
        "                  axi.yaxis.set_ticklabels([])\n",
        "                  axi.grid(False)\n",
        "                  axes2_list.append(axi)\n",
        "                  r_value = average_red_value_from_tensor(images2[i])\n",
        "                  #axi.set_xlabel(\"{:.2f}\".format(r_value), fontsize = 10)\n",
        "\n",
        "            if images1 is not None and images2 is not None:\n",
        "              axes1_list[0].annotate('Reference',\n",
        "              xy=(0.0, 0.307), xycoords='figure fraction',\n",
        "              horizontalalignment='left', verticalalignment='center')\n",
        "              axes2_list[0].annotate('Generated Images',\n",
        "              xy=(0.0, 0.12), xycoords='figure fraction',\n",
        "              horizontalalignment='left', verticalalignment='center')\n",
        "\n",
        "            if images2 is None and images1 is not None:\n",
        "              axes1_list[0].annotate('Generated Images',\n",
        "              xy=(0.0, 0.22), xycoords='figure fraction',\n",
        "              horizontalalignment='left', verticalalignment='center')\n",
        "\n",
        "            #fig.subplots_adjust(hspace = 0.0)  # Adjust this value to control the space\n",
        "            #fig.tight_layout()\n",
        "\n",
        "\n",
        "            if save_path is not None:\n",
        "              timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "              file_name = f\"GT_and_Model_plot_{timestr}.png\"\n",
        "              fig.savefig(f\"{save_path}/{file_name}\", dpi = dpi_save, bbox_inches='tight')\n",
        "\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbBF2QYeWHWl"
      },
      "outputs": [],
      "source": [
        "from templates import *\n",
        "\n",
        "def interpolate_quadrants(q1, q2, prediction_model, images = images, T_encoding = 2, T_decoding = 250, n_datapoints = 1000, n_images = 20):\n",
        "  \"\"\"\n",
        "  Interpolates between images where the white square is in quadrant q1 and quadrant q2.\n",
        "  params:\n",
        "    q1: quadrant for first image\n",
        "    q2: quadrant for last image\n",
        "    T_encoding: number of steps for the stochastic encoder\n",
        "    T_decoding: number of steps for the stochastic decoder\n",
        "  \"\"\"\n",
        "\n",
        "  img1 = images[:999][labels[:999] == q1][0]\n",
        "  img2 = images[:999][labels[:999] == q2][0]\n",
        "\n",
        "  batch = torch.stack([\n",
        "    img1,\n",
        "    img2,\n",
        "  ])\n",
        "\n",
        "  cond = model.encode(batch.to(device))\n",
        "  xT = model.encode_stochastic(batch.to(device), cond, T=T_encoding)\n",
        "\n",
        "  alpha = torch.tensor(np.linspace(0, 1, n_images, dtype=np.float32)).to(cond.device)\n",
        "  intp = cond[0][None] * (1 - alpha[:, None]) + cond[1][None] * alpha[:, None]\n",
        "\n",
        "  def cos(a, b):\n",
        "      a = a.view(-1)\n",
        "      b = b.view(-1)\n",
        "      a = F.normalize(a, dim=0)\n",
        "      b = F.normalize(b, dim=0)\n",
        "      return (a * b).sum()\n",
        "\n",
        "  theta = torch.arccos(cos(xT[0], xT[1]))\n",
        "  x_shape = xT[0].shape\n",
        "  intp_x = (torch.sin((1 - alpha[:, None]) * theta) * xT[0].flatten(0, 2)[None] + torch.sin(alpha[:, None] * theta) * xT[1].flatten(0, 2)[None]) / torch.sin(theta)\n",
        "  intp_x = intp_x.view(-1, *x_shape)\n",
        "\n",
        "  images = model.render(intp_x, intp, T=T_decoding)\n",
        "\n",
        "\n",
        "  preds_images = prediction_model(intp)[:, 0].detach().cpu().numpy()\n",
        "\n",
        "  alpha_detailed = torch.tensor(np.linspace(0, 1, n_datapoints, dtype=np.float32)).to(cond.device)\n",
        "  intp_detailed = cond[0][None] * (1 - alpha_detailed[:, None]) + cond[1][None] * alpha_detailed[:, None]\n",
        "\n",
        "  preds_detailed = prediction_model(intp)[:, 0].detach().cpu().numpy()\n",
        "\n",
        "  plot_images_and_predictions(images, preds_images,preds_detailed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVBEO33XLLD_"
      },
      "outputs": [],
      "source": [
        "def interpolate_images(img1, img2, prediction_model, images = images, T_encoding = 2, T_decoding = 200, n_datapoints = 1000, n_images = 10):\n",
        "  \"\"\"\n",
        "  Interpolates between images where the white square is in quadrant q1 and quadrant q2.\n",
        "  params:\n",
        "    img1: first image\n",
        "    img2: second image\n",
        "    T_encoding: number of steps for the stochastic encoder\n",
        "    T_decoding: number of steps for the stochastic decoder\n",
        "  \"\"\"\n",
        "\n",
        "  batch = torch.stack([\n",
        "    img1,\n",
        "    img2,\n",
        "  ])\n",
        "\n",
        "  cond = model.encode(batch.to(device))\n",
        "  xT = model.encode_stochastic(batch.to(device), cond, T=T_encoding)\n",
        "\n",
        "  alpha = torch.tensor(np.linspace(0, 1, n_images, dtype=np.float32)).to(cond.device)\n",
        "  intp = cond[0][None] * (1 - alpha[:, None]) + cond[1][None] * alpha[:, None]\n",
        "\n",
        "  def cos(a, b):\n",
        "      a = a.view(-1)\n",
        "      b = b.view(-1)\n",
        "      a = F.normalize(a, dim=0)\n",
        "      b = F.normalize(b, dim=0)\n",
        "      return (a * b).sum()\n",
        "\n",
        "  theta = torch.arccos(cos(xT[0], xT[1]))\n",
        "  x_shape = xT[0].shape\n",
        "  intp_x = (torch.sin((1 - alpha[:, None]) * theta) * xT[0].flatten(0, 2)[None] + torch.sin(alpha[:, None] * theta) * xT[1].flatten(0, 2)[None]) / torch.sin(theta)\n",
        "  intp_x = intp_x.view(-1, *x_shape)\n",
        "\n",
        "  images = model.render(intp_x, intp, T=T_decoding)\n",
        "\n",
        "\n",
        "  preds_images = prediction_model(intp)[:, 0].detach().cpu().numpy()\n",
        "  #preds_images = preds_images - np.mean(preds_images)\n",
        "\n",
        "\n",
        "  alpha_detailed = torch.tensor(np.linspace(0, 1, n_datapoints, dtype=np.float32)).to(cond.device)\n",
        "  intp_detailed = cond[0][None] * (1 - alpha_detailed[:, None]) + cond[1][None] * alpha_detailed[:, None]\n",
        "\n",
        "  preds_detailed = prediction_model(intp_detailed)[:, 0].detach().cpu().numpy()\n",
        "\n",
        "  mean = np.mean(preds_detailed)\n",
        "\n",
        "  preds_detailed = preds_detailed - mean\n",
        "  preds_images = preds_images - mean\n",
        "\n",
        "  #plot_images_and_predictions(images, preds_images,preds_detailed)\n",
        "\n",
        "  return images, preds_images,preds_detailed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOToOlJHJZxC"
      },
      "outputs": [],
      "source": [
        "def interpolate_images_gt(img1, img2, feature_extraction_fun = average_red_value_from_tensor, prediction_fun = f_img_raw, images = images, n_datapoints = 1000, n_images= 10):\n",
        "  #if img1.shape[-1] != 3:\n",
        "  #  img1 = img1.permute(1, 2, 0)\n",
        "   # img2 = img2.permute(1,2,0)\n",
        "\n",
        "  interpolated_images= interpolate_images_linear_torch(img1, img2, n_images)\n",
        "  interpolated_images = [img/2 +0.5 for img in interpolated_images]\n",
        "\n",
        "  latent_1 = standardize_x_vals(feature_extraction_fun(img1))\n",
        "  latent_2 = standardize_x_vals(feature_extraction_fun(img2))\n",
        "\n",
        "  print(latent_1, latent_2)\n",
        "\n",
        "\n",
        "\n",
        "  latent_intp = np.linspace(latent_1, latent_2, n_datapoints)\n",
        "  #latent_intp = (latent_intp - np.min(latent_intp))/(np.max(latent_intp) - np.min(latent_intp))\n",
        "\n",
        "  preds_smooth = np.array([prediction_fun(latent_i).item() for latent_i in latent_intp])\n",
        "\n",
        "  interval = n_datapoints // (n_images - 1)\n",
        "\n",
        "  # Generate the indices of the selected points\n",
        "  selected_indices = [i * interval for i in range(n_images - 1)] + [n_datapoints - 1]\n",
        "\n",
        "  preds_images = preds_smooth[selected_indices]\n",
        "\n",
        "  mean = np.mean(preds_smooth)\n",
        "  preds_smooth = preds_smooth - mean\n",
        "  preds_images = preds_images -mean\n",
        "\n",
        "  #interpolated_images = torch.stack(interpolated_images)\n",
        "\n",
        "  #plot_images_and_predictions(interpolated_images, image_preds = preds_images, preds_smooth = preds_smooth)\n",
        "\n",
        "  return interpolated_images, preds_images, preds_smooth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQOjXxBiJ-ww"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVWmx_SdFE8O"
      },
      "outputs": [],
      "source": [
        "def interpolate_images_gt_and_model(img1,\n",
        "                                    img2,\n",
        "                                    prediction_model,\n",
        "                                    feature_extraction_fun = average_red_value_from_tensor,\n",
        "                                    prediction_fun = f_img_raw,\n",
        "                                    images = images,\n",
        "                                    n_datapoints = 1000,\n",
        "                                    n_images= 10,\n",
        "                                    T_encoding = 2,\n",
        "                                    T_decoding = 200,\n",
        "                                    ymin=-1.3,\n",
        "                                    ymax=1.3,\n",
        "                                    save_path = None\n",
        "                                    ):\n",
        "  interpolated_images_gt, preds_images_gt, preds_smooth_gt = interpolate_images_gt(\n",
        "      img1,\n",
        "      img2,\n",
        "      feature_extraction_fun = feature_extraction_fun,\n",
        "      prediction_fun = prediction_fun,\n",
        "      images = images,\n",
        "      n_datapoints = n_datapoints,\n",
        "      n_images = n_images\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "  interpolated_images_mod, preds_images_mod, preds_smooth_mod = interpolate_images(img1,\n",
        "                                 img2,\n",
        "                                 prediction_model,\n",
        "                                 images = images,\n",
        "                                 T_encoding = T_encoding,\n",
        "                                 T_decoding = T_decoding,\n",
        "                                 n_datapoints = n_datapoints,\n",
        "                                 n_images = n_images)\n",
        "\n",
        "\n",
        "  plot_images_and_predictions2(images1 = interpolated_images_gt,\n",
        "                               image_preds1 = preds_images_gt,\n",
        "                               preds_smooth1 = preds_smooth_gt,\n",
        "                               images2 =interpolated_images_mod ,\n",
        "                               image_preds2 = preds_images_mod,\n",
        "                               preds_smooth2 = preds_smooth_mod,\n",
        "                               ymin=ymin,\n",
        "                               ymax=ymax,\n",
        "                               save_path = save_path\n",
        "                               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJ6A3Ym2N7v5"
      },
      "outputs": [],
      "source": [
        "cnam_mlp_plus_bias = lambda x: cnam.mlp(x) + cnam.feat_nam.bias.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdkzrX_UX06D"
      },
      "outputs": [],
      "source": [
        "save_path_int = f\"{result_path}/Interpolation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "qCHUl-zbQrGd",
        "outputId": "d1f55368-9642-4ec4-8893-a8bd6b54530b"
      },
      "outputs": [],
      "source": [
        "#img1 = generate_img(0.1)\n",
        "#img2 = generate_img(0.9)\n",
        "#print(interpolate_images_linear_torch(img1, img2).shape)\n",
        "\n",
        "img1 = generate_img(0.2, g = 0.5, b = 0.5)\n",
        "img2 = generate_img(0.7, g = 0.5, b = 0.5)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=[6, 8] )\n",
        "ax[0].imshow(img1.permute(1, 2, 0).cpu())\n",
        "ax[1].imshow(img2.permute(1, 2, 0).cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e4ZeYwe1Mhps",
        "outputId": "09b1d8bc-e5cc-4514-8c1e-4e613ce7565a"
      },
      "outputs": [],
      "source": [
        "interpolate_images_gt_and_model(img1, img2, prediction_model = cnam_mlp_plus_bias, T_encoding = 250, T_decoding = 250, n_images = 15, save_path = save_path_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "O_INHQs1RINi",
        "outputId": "bd86f509-680d-4910-ec46-aa4d58ccc34d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAECCAYAAACIWG9sAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAaFElEQVR4nO3df2yV9f3+8aul7Wm17SmtcA4NLXQZW3HIhkXKEfYLuzXMOBjVqcGIhkhkB5Q2i7aJ4lyc7TQbjIUf07iimYyNP0AxEWLqrDErxdawiY6Kk6TVcg66recUZk8J5/35gy8nHsHvOO351fN+PpI7ofd9n7uvvsNFLs65z2mWMcYIAABYJTvVAwAAgOSjAAAAYCEKAAAAFqIAAABgIQoAAAAWogAAAGAhCgAAABaiAAAAYCEKAAAAFqIAAABgoYQVgK1bt2rmzJnKz89XbW2tDh8+nKhvBSANkHlgYslKxO8C+NOf/qQ777xTO3bsUG1trTZv3qw9e/aor69PU6dO/f8+NhwOa3BwUEVFRcrKyor3aIBVjDEaHh5WeXm5srMT94TfeDIvkXsgXmLKvEmABQsWGK/XG/n63Llzpry83LS2tv7Pxw4MDBhJbGxscdwGBgYSEfWI8WTeGHLPxhbv7XIyn6M4Gx0dVW9vr1paWiL7srOzVVdXp66urovOD4VCCoVCka/N/3tCYkpjo7IdjniPB1glHArp402bVFRUlLDvEWvmpS/OfU9jowrJPTBmp0Mhzb/MzMe9AHzyySc6d+6cXC5X1H6Xy6Vjx45ddH5ra6seffTRi/ZnOxwUACBOEvm0eqyZl74494UOh4rIPTBul5P5lL8LoKWlRYFAILINDAykeiQACUbugdSL+zMAV111lSZNmiS/3x+13+/3y+12X3S+w+GQg8YPTFixZl4i90A6iPszAHl5eaqpqVFHR0dkXzgcVkdHhzweT7y/HYAUI/PAxBT3ZwAkqampSatWrdL8+fO1YMECbd68WWfOnNHdd9+diG8HIMXIPDDxJKQA3Hrrrfr444+1ceNG+Xw+feMb39CBAwcuukkIQGYg88DEk5ACIEnr1q3TunXrEnV5AGmGzAMTS8rfBQAAAJKPAgAAgIUoAAAAWIgCAACAhSgAAABYiAIAAICFKAAAAFiIAgAAgIUoAAAAWIgCAACAhSgAAABYiAIAAICFKAAAAFiIAgAAgIUoAAAAWIgCAACAhSgAAABYiAIAAICFKAAAAFiIAgAAgIUoAAAAWIgCAACAhWIuAK+//rpuuukmlZeXKysrS/v27Ys6bozRxo0bNW3aNBUUFKiurk7Hjx+P17wAkozMA5kp5gJw5swZff3rX9fWrVsvefyJJ57Qli1btGPHDnV3d+vKK69UfX29RkZGxj0sgOQj80Bmyon1AUuXLtXSpUsvecwYo82bN+uhhx7SsmXLJEnPPfecXC6X9u3bp9tuu2180wJIOjIPZKa43gNw4sQJ+Xw+1dXVRfY5nU7V1taqq6vrko8JhUIKBoNRG4CJYSyZl8g9kA7iWgB8Pp8kyeVyRe13uVyRY5/X2toqp9MZ2SoqKuI5EoAEGkvmJXIPpIOUvwugpaVFgUAgsg0MDKR6JAAJRu6B1ItrAXC73ZIkv98ftd/v90eOfZ7D4VBxcXHUBmBiGEvmJXIPpIO4FoCqqiq53W51dHRE9gWDQXV3d8vj8cTzWwFIA2QemLhifhfA6dOn9f7770e+PnHihI4cOaLS0lJVVlZqw4YNeuyxxzRr1ixVVVXp4YcfVnl5uZYvXx7PuQEkCZkHMlPMBaCnp0ff/e53I183NTVJklatWqWdO3fqgQce0JkzZ7RmzRoNDQ1p8eLFOnDggPLz8+M3NYCkIfNAZsoyxphUD/FZwWBQTqdTruZmZTscqR4HmNDCoZD8bW0KBAJp/Tr7hdwfa25WEbkHxmw4FFL1ZWY+5e8CAAAAyUcBAADAQhQAAAAsRAEAAMBCFAAAACxEAQAAwEIUAAAALEQBAADAQhQAAAAsRAEAAMBCFAAAACxEAQAAwEIUAAAALEQBAADAQhQAAAAsRAEAAMBCFAAAACxEAQAAwEIUAAAALEQBAADAQhQAAAAsRAEAAMBCMRWA1tZWXXfddSoqKtLUqVO1fPly9fX1RZ0zMjIir9ersrIyFRYWqqGhQX6/P65DA0gecg9kppgKQGdnp7xerw4dOqRXXnlFZ8+e1fe//32dOXMmck5jY6P279+vPXv2qLOzU4ODg1qxYkXcBweQHOQeyEw5sZx84MCBqK937typqVOnqre3V9/61rcUCAT0zDPPaNeuXVqyZIkkqb29XbNnz9ahQ4e0cOHC+E0OICnIPZCZxnUPQCAQkCSVlpZKknp7e3X27FnV1dVFzqmurlZlZaW6uroueY1QKKRgMBi1AUhf5B7IDGMuAOFwWBs2bNCiRYs0Z84cSZLP51NeXp5KSkqiznW5XPL5fJe8Tmtrq5xOZ2SrqKgY60gAEozcA5ljzAXA6/Xq6NGj2r1797gGaGlpUSAQiGwDAwPjuh6AxCH3QOaI6R6AC9atW6eXXnpJr7/+uqZPnx7Z73a7NTo6qqGhoaj/Dfj9frnd7ktey+FwyOFwjGUMAElE7oHMEtMzAMYYrVu3Tnv37tWrr76qqqqqqOM1NTXKzc1VR0dHZF9fX5/6+/vl8XjiMzGApCL3QGaK6RkAr9erXbt26YUXXlBRUVHk9T2n06mCggI5nU6tXr1aTU1NKi0tVXFxsdavXy+Px8OdwMAERe6BzBRTAdi+fbsk6Tvf+U7U/vb2dt11112SpE2bNik7O1sNDQ0KhUKqr6/Xtm3b4jIsgOQj90BmiqkAGGP+5zn5+fnaunWrtm7dOuahAKQPcg9kJn4XAAAAFqIAAABgIQoAAAAWogAAAGAhCgAAABaiAAAAYCEKAAAAFqIAAABgIQoAAAAWogAAAGAhCgAAABaiAAAAYCEKAAAAFqIAAABgIQoAAAAWogAAAGAhCgAAABaiAAAAYCEKAAAAFqIAAABgIQoAAAAWogAAAGChmArA9u3bNXfuXBUXF6u4uFgej0cvv/xy5PjIyIi8Xq/KyspUWFiohoYG+f3+uA8NIHnIPZCZYioA06dPV1tbm3p7e9XT06MlS5Zo2bJleueddyRJjY2N2r9/v/bs2aPOzk4NDg5qxYoVCRkcQHKQeyAzZRljzHguUFpaqieffFI333yzpkyZol27dunmm2+WJB07dkyzZ89WV1eXFi5ceFnXCwaDcjqdcjU3K9vhGM9ogPXCoZD8bW0KBAIqLi6O23UTlftjzc0qIvfAmA2HQqq+zMyP+R6Ac+fOaffu3Tpz5ow8Ho96e3t19uxZ1dXVRc6prq5WZWWlurq6vvA6oVBIwWAwagOQnsg9kDliLgBvv/22CgsL5XA4dO+992rv3r26+uqr5fP5lJeXp5KSkqjzXS6XfD7fF16vtbVVTqczslVUVMT8QwBILHIPZJ6YC8BXv/pVHTlyRN3d3Vq7dq1WrVqld999d8wDtLS0KBAIRLaBgYExXwtAYpB7IPPkxPqAvLw8ffnLX5Yk1dTU6M0339RvfvMb3XrrrRodHdXQ0FDU/wb8fr/cbvcXXs/hcMjBa35AWiP3QOYZ9+cAhMNhhUIh1dTUKDc3Vx0dHZFjfX196u/vl8fjGe+3AZBGyD0w8cX0DEBLS4uWLl2qyspKDQ8Pa9euXXrttdd08OBBOZ1OrV69Wk1NTSotLVVxcbHWr18vj8dz2XcCA0g/5B7ITDEVgFOnTunOO+/UyZMn5XQ6NXfuXB08eFDf+973JEmbNm1Sdna2GhoaFAqFVF9fr23btiVkcADJQe6BzDTuzwGINz4HAIifRH0OQLzxOQBAfCTlcwAAAMDERQEAAMBCFAAAACxEAQAAwEIUAAAALEQBAADAQhQAAAAsRAEAAMBCFAAAACxEAQAAwEIUAAAALEQBAADAQhQAAAAsRAEAAMBCFAAAACxEAQAAwEIUAAAALEQBAADAQhQAAAAsRAEAAMBCFAAAACxEAQAAwELjKgBtbW3KysrShg0bIvtGRkbk9XpVVlamwsJCNTQ0yO/3j3dOAGmAzAOZY8wF4M0339Tvfvc7zZ07N2p/Y2Oj9u/frz179qizs1ODg4NasWLFuAcFkFpkHsgsYyoAp0+f1sqVK/X0009r8uTJkf2BQEDPPPOMfv3rX2vJkiWqqalRe3u7/vrXv+rQoUNxGxpAcpF5IPOMqQB4vV7deOONqquri9rf29urs2fPRu2vrq5WZWWlurq6LnmtUCikYDAYtQFIL/HMvETugXSQE+sDdu/erbfeektvvvnmRcd8Pp/y8vJUUlIStd/lcsnn813yeq2trXr00UdjHQNAksQ78xK5B9JBTM8ADAwM6P7779fzzz+v/Pz8uAzQ0tKiQCAQ2QYGBuJyXQDjl4jMS+QeSAcxFYDe3l6dOnVK1157rXJycpSTk6POzk5t2bJFOTk5crlcGh0d1dDQUNTj/H6/3G73Ja/pcDhUXFwctQFID4nIvETugXQQ00sAN9xwg95+++2ofXfffbeqq6v14IMPqqKiQrm5uero6FBDQ4Mkqa+vT/39/fJ4PPGbGkBSkHkgc8VUAIqKijRnzpyofVdeeaXKysoi+1evXq2mpiaVlpaquLhY69evl8fj0cKFC+M3NYCkIPNA5or5JsD/ZdOmTcrOzlZDQ4NCoZDq6+u1bdu2eH8bAGmCzAMTU5YxxqR6iM8KBoNyOp1yNTcr2+FI9TjAhBYOheRva1MgEEjr19kv5P5Yc7OKyD0wZsOhkKovM/P8LgAAACxEAQAAwEIUAAAALEQBAADAQhQAAAAsRAEAAMBCFAAAACxEAQAAwEIUAAAALEQBAADAQhQAAAAsRAEAAMBCFAAAACxEAQAAwEIUAAAALEQBAADAQhQAAAAsRAEAAMBCFAAAACxEAQAAwEIUAAAALEQBAADAQjEVgJ/97GfKysqK2qqrqyPHR0ZG5PV6VVZWpsLCQjU0NMjv98d9aADJQ+6BzBTzMwBf+9rXdPLkycj2xhtvRI41NjZq//792rNnjzo7OzU4OKgVK1bEdWAAyUfugcyTE/MDcnLkdrsv2h8IBPTMM89o165dWrJkiSSpvb1ds2fP1qFDh7Rw4cLxTwsgJcg9kHlifgbg+PHjKi8v15e+9CWtXLlS/f39kqTe3l6dPXtWdXV1kXOrq6tVWVmprq6uL7xeKBRSMBiM2gCkF3IPZJ6YCkBtba127typAwcOaPv27Tpx4oS++c1vanh4WD6fT3l5eSopKYl6jMvlks/n+8Jrtra2yul0RraKioox/SAAEoPcA5kpppcAli5dGvnz3LlzVVtbqxkzZujPf/6zCgoKxjRAS0uLmpqaIl8Hg0H+MQDSCLkHMtO43gZYUlKir3zlK3r//ffldrs1OjqqoaGhqHP8fv8lXzu8wOFwqLi4OGoDkL7IPZAZxlUATp8+rX/+85+aNm2aampqlJubq46Ojsjxvr4+9ff3y+PxjHtQAOmB3AOZIaaXAH7605/qpptu0owZMzQ4OKhHHnlEkyZN0u233y6n06nVq1erqalJpaWlKi4u1vr16+XxeLgTGJjAyD2QmWIqAB9++KFuv/12/etf/9KUKVO0ePFiHTp0SFOmTJEkbdq0SdnZ2WpoaFAoFFJ9fb22bduWkMEBJAe5BzJTljHGpHqIzwoGg3I6nXI1Nyvb4Uj1OMCEFg6F5G9rUyAQSOvX2S/k/lhzs4rIPTBmw6GQqi8z8/wuAAAALEQBAADAQhQAAAAsRAEAAMBCFAAAACxEAQAAwEIUAAAALEQBAADAQhQAAAAsRAEAAMBCFAAAACxEAQAAwEIUAAAALEQBAADAQhQAAAAsRAEAAMBCFAAAACxEAQAAwEIUAAAALEQBAADAQhQAAAAsRAEAAMBCMReAjz76SHfccYfKyspUUFCga665Rj09PZHjxhht3LhR06ZNU0FBgerq6nT8+PG4Dg0gucg9kHliKgD/+c9/tGjRIuXm5urll1/Wu+++q1/96leaPHly5JwnnnhCW7Zs0Y4dO9Td3a0rr7xS9fX1GhkZifvwABKP3AOZKSeWk3/5y1+qoqJC7e3tkX1VVVWRPxtjtHnzZj300ENatmyZJOm5556Ty+XSvn37dNttt8VpbADJQu6BzBTTMwAvvvii5s+fr1tuuUVTp07VvHnz9PTTT0eOnzhxQj6fT3V1dZF9TqdTtbW16urquuQ1Q6GQgsFg1AYgfZB7IDPFVAA++OADbd++XbNmzdLBgwe1du1a3XfffXr22WclST6fT5LkcrmiHudyuSLHPq+1tVVOpzOyVVRUjOXnAJAg5B7ITDEVgHA4rGuvvVaPP/645s2bpzVr1uiee+7Rjh07xjxAS0uLAoFAZBsYGBjztQDEH7kHMlNMBWDatGm6+uqro/bNnj1b/f39kiS32y1J8vv9Uef4/f7Isc9zOBwqLi6O2gCkD3IPZKaYCsCiRYvU19cXte+9997TjBkzJJ2/McjtdqujoyNyPBgMqru7Wx6PJw7jAkg2cg9kppjeBdDY2Kjrr79ejz/+uH784x/r8OHDeuqpp/TUU09JkrKysrRhwwY99thjmjVrlqqqqvTwww+rvLxcy5cvT8T8ABKM3AOZKaYCcN1112nv3r1qaWnRz3/+c1VVVWnz5s1auXJl5JwHHnhAZ86c0Zo1azQ0NKTFixfrwIEDys/Pj/vwABKP3AOZKcsYY1I9xGcFg0E5nU65mpuV7XCkehxgQguHQvK3tSkQCKT16+wXcn+suVlF5B4Ys+FQSNWXmXl+FwAAABaiAAAAYCEKAAAAForpJsBkuHBLQjgUSvEkwMR3IUdpdqvPRS7Md5rcA+NyOobMp91NgB9++CEfCwrE2cDAgKZPn57qMb4QuQfi63Iyn3YFIBwOa3BwUMYYVVZWamBgIK3vXk60YDCoiooK1oF1kBT7OhhjNDw8rPLycmVnp+8rfuQ+Gn/fz2MdzotlHWLJfNq9BJCdna3p06dHfjsYHxN6HutwHutwXizr4HQ6EzzN+JH7S2MdzmMdzrvcdbjczKfvfwkAAEDCUAAAALBQ2hYAh8OhRx55RA7LPxWMdTiPdTgv09ch03++y8U6nMc6nJeodUi7mwABAEDipe0zAAAAIHEoAAAAWIgCAACAhSgAAABYiAIAAICF0rIAbN26VTNnzlR+fr5qa2t1+PDhVI+UUK2trbruuutUVFSkqVOnavny5err64s6Z2RkRF6vV2VlZSosLFRDQ4P8fn+KJk6OtrY2ZWVlacOGDZF9tqzDRx99pDvuuENlZWUqKCjQNddco56enshxY4w2btyoadOmqaCgQHV1dTp+/HgKJx4/ck/uJXKf1NybNLN7926Tl5dnfv/735t33nnH3HPPPaakpMT4/f5Uj5Yw9fX1pr293Rw9etQcOXLE/OAHPzCVlZXm9OnTkXPuvfdeU1FRYTo6OkxPT49ZuHChuf7661M4dWIdPnzYzJw508ydO9fcf//9kf02rMO///1vM2PGDHPXXXeZ7u5u88EHH5iDBw+a999/P3JOW1ubcTqdZt++feZvf/ub+eEPf2iqqqrMp59+msLJx47ck3tjyH2yc592BWDBggXG6/VGvj537pwpLy83ra2tKZwquU6dOmUkmc7OTmOMMUNDQyY3N9fs2bMncs4//vEPI8l0dXWlasyEGR4eNrNmzTKvvPKK+fa3vx35h8CWdXjwwQfN4sWLv/B4OBw2brfbPPnkk5F9Q0NDxuFwmD/+8Y/JGDHuyD25J/fJz31avQQwOjqq3t5e1dXVRfZlZ2errq5OXV1dKZwsuQKBgCSptLRUktTb26uzZ89GrUt1dbUqKyszcl28Xq9uvPHGqJ9XsmcdXnzxRc2fP1+33HKLpk6dqnnz5unpp5+OHD9x4oR8Pl/UOjidTtXW1k7IdSD355F7cp/s3KdVAfjkk0907tw5uVyuqP0ul0s+ny9FUyVXOBzWhg0btGjRIs2ZM0eS5PP5lJeXp5KSkqhzM3Fddu/erbfeekutra0XHbNlHT744ANt375ds2bN0sGDB7V27Vrdd999evbZZyUp8rNmSk7IPbkn96nJfdr9OmDbeb1eHT16VG+88UaqR0m6gYEB3X///XrllVeUn5+f6nFSJhwOa/78+Xr88cclSfPmzdPRo0e1Y8cOrVq1KsXTIRHIPblPRe7T6hmAq666SpMmTbro7k6/3y+3252iqZJn3bp1eumll/SXv/xF06dPj+x3u90aHR3V0NBQ1PmZti69vb06deqUrr32WuXk5CgnJ0ednZ3asmWLcnJy5HK5rFiHadOm6eqrr47aN3v2bPX390tS5GfNlJyQe3JP7lOT+7QqAHl5eaqpqVFHR0dkXzgcVkdHhzweTwonSyxjjNatW6e9e/fq1VdfVVVVVdTxmpoa5ebmRq1LX1+f+vv7M2pdbrjhBr399ts6cuRIZJs/f75WrlwZ+bMN67Bo0aKL3g723nvvacaMGZKkqqoqud3uqHUIBoPq7u6ekOtA7sk9uU9R7sd062AC7d692zgcDrNz507z7rvvmjVr1piSkhLj8/lSPVrCrF271jidTvPaa6+ZkydPRrb//ve/kXPuvfdeU1lZaV599VXT09NjPB6P8Xg8KZw6OT57N7AxdqzD4cOHTU5OjvnFL35hjh8/bp5//nlzxRVXmD/84Q+Rc9ra2kxJSYl54YUXzN///nezbNmyCf82QHJP7i8g98nJfdoVAGOM+e1vf2sqKytNXl6eWbBggTl06FCqR0ooSZfc2tvbI+d8+umn5ic/+YmZPHmyueKKK8yPfvQjc/LkydQNnSSf/4fAlnXYv3+/mTNnjnE4HKa6uto89dRTUcfD4bB5+OGHjcvlMg6Hw9xwww2mr68vRdPGB7kn9xeQ++TkPssYY8b23AEAAJio0uoeAAAAkBwUAAAALEQBAADAQhQAAAAsRAEAAMBCFAAAACxEAQAAwEIUAAAALEQBAADAQhQAAAAsRAEAAMBC/wfhukXGriiUaAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 600x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#img1 = generate_img(0.1)\n",
        "#img2 = generate_img(0.9)\n",
        "#print(interpolate_images_linear_torch(img1, img2).shape)\n",
        "\n",
        "img1 = generate_img(0.1, g = 0.5, b = 0.5)\n",
        "img2 = generate_img(0.9, g = 0.5, b = 0.5)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=[6, 8] )\n",
        "ax[0].imshow(img1.permute(1, 2, 0).cpu())\n",
        "ax[1].imshow(img2.permute(1, 2, 0).cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-qelRcVdRINi",
        "outputId": "793cd4af-68cc-42ba-b274-0d90176656a4"
      },
      "outputs": [],
      "source": [
        "interpolate_images_gt_and_model(img1, img2, prediction_model = cnam_mlp_plus_bias, T_encoding = 250, T_decoding = 250, n_images = 15, save_path = save_path_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "yXjdWQIOQSrs",
        "outputId": "71d24c24-7421-4d65-aae2-d61b91d38d13"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#img1 = generate_img(0.1)\n",
        "#img2 = generate_img(0.9)\n",
        "#print(interpolate_images_linear_torch(img1, img2).shape)\n",
        "\n",
        "img1 = generate_img(0.5, g = 0.2, b = 0.5)\n",
        "img2 = generate_img(0.5, g = 0.7, b = 0.5)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=[6, 8] )\n",
        "ax[0].imshow(img1.permute(1, 2, 0).cpu())\n",
        "ax[1].imshow(img2.permute(1, 2, 0).cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J7YzZ4GxQSrt",
        "outputId": "0651e97f-6335-44ce-89dc-0b8108eab973"
      },
      "outputs": [],
      "source": [
        "interpolate_images_gt_and_model(img1, img2, prediction_model = cnam_mlp_plus_bias, T_encoding = 250, T_decoding = 250, n_images = 15, save_path = save_path_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "U18XzrUjQZpo",
        "outputId": "24907c47-3f1b-44f6-cd1c-1ca6bb658c6d"
      },
      "outputs": [],
      "source": [
        "#img1 = generate_img(0.1)\n",
        "#img2 = generate_img(0.9)\n",
        "#print(interpolate_images_linear_torch(img1, img2).shape)\n",
        "\n",
        "img1 = generate_img(0.5, g = 0.5, b = 0.2)\n",
        "img2 = generate_img(0.5, g = 0.5, b = 0.7)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=[6, 8] )\n",
        "ax[0].imshow(img1.permute(1, 2, 0).cpu())\n",
        "ax[1].imshow(img2.permute(1, 2, 0).cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oRUgRKqqQZpq",
        "outputId": "557a0566-edfe-4352-dade-9e0449e2a5a1"
      },
      "outputs": [],
      "source": [
        "interpolate_images_gt_and_model(img1, img2, prediction_model = cnam_mlp_plus_bias, T_encoding = 250, T_decoding = 250, n_images = 15, save_path = save_path_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "rMuYqODlQxgm",
        "outputId": "4aad7818-bdbe-41f0-8490-2064dded96b1"
      },
      "outputs": [],
      "source": [
        "#img1 = generate_img(0.1)\n",
        "#img2 = generate_img(0.9)\n",
        "#print(interpolate_images_linear_torch(img1, img2).shape)\n",
        "\n",
        "img1 = generate_img(0.3, g = 0.1, b = 0.5)\n",
        "img2 = generate_img(0.6, g = 0.5, b = 0.5)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=[6, 8] )\n",
        "ax[0].imshow(img1.permute(1, 2, 0).cpu())\n",
        "ax[1].imshow(img2.permute(1, 2, 0).cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v68Xf00WRc6n",
        "outputId": "598939e6-7e0b-448c-87bb-a62098877ab7"
      },
      "outputs": [],
      "source": [
        "interpolate_images_gt_and_model(img1, img2, prediction_model = cnam_mlp_plus_bias, T_encoding = 250, T_decoding = 250, n_images = 15, save_path = save_path_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgtOQa-1GVtl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP3iT6HLX-2d"
      },
      "outputs": [],
      "source": [
        "class ColorSimulator:\n",
        "    \"\"\"\n",
        "    Simulate images that are fully in one random color.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, num_images, seed = 43):\n",
        "        \"\"\"\n",
        "        :param size: Size of the images.\n",
        "        :param num_images: Number of images to simulate.\n",
        "        \"\"\"\n",
        "        self.size = size\n",
        "        self.num_images = num_images\n",
        "        self.seed = seed\n",
        "        torch.manual_seed(self.seed)\n",
        "\n",
        "    def simulate(self):\n",
        "        \"\"\"\n",
        "        Simulate the images.\n",
        "        :return: Tensor of shape (num_images, 3, size, size) containing the images\n",
        "        \"\"\"\n",
        "        # Create a tensor to hold the images\n",
        "        images = torch.zeros(self.num_images, 3, self.size, self.size)\n",
        "        # Generate random colors\n",
        "        colors = torch.rand((self.num_images, 3))\n",
        "        # Assign the colors to the images\n",
        "        images = images + colors[:, :, None, None]\n",
        "        return images\n",
        "\n",
        "test_image_simulator = ColorSimulator(size = 64, num_images = 1000)\n",
        "test_images = test_image_simulator.simulate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cStBOLluQDfk"
      },
      "outputs": [],
      "source": [
        "def score_prediction_ground_truth(img1,\n",
        "                                  img2,\n",
        "                                  prediction_model,\n",
        "                                  feature_extraction_fun = average_red_value_from_tensor,\n",
        "                                  prediction_fun = f_img_raw,\n",
        "                                  images = images,\n",
        "                                  n_datapoints = 1000,\n",
        "                                  n_images= 10,\n",
        "                                  T_encoding = 2,\n",
        "                                  T_decoding = 2,\n",
        "                                  ymin=-1.3,\n",
        "                                  ymax=1.3,\n",
        "                                  save_path = None\n",
        "                                  ):\n",
        "  interpolated_images_gt, preds_images_gt, preds_smooth_gt = interpolate_images_gt(\n",
        "      img1,\n",
        "      img2,\n",
        "      feature_extraction_fun = feature_extraction_fun,\n",
        "      prediction_fun = prediction_fun,\n",
        "      images = images,\n",
        "      n_datapoints = n_datapoints,\n",
        "      n_images = n_images\n",
        "  )\n",
        "\n",
        "  interpolated_images_mod, preds_images_mod, preds_smooth_mod = interpolate_images(img1,\n",
        "                                 img2,\n",
        "                                 prediction_model,\n",
        "                                 images = images,\n",
        "                                 T_encoding = T_encoding,\n",
        "                                 T_decoding = T_decoding,\n",
        "                                 n_datapoints = n_datapoints,\n",
        "                                 n_images = n_images)\n",
        "\n",
        "  preds_smooth_gt = torch.tensor(preds_smooth_gt)\n",
        "  preds_smooth_mod = torch.tensor(preds_smooth_mod)\n",
        "\n",
        "\n",
        "  var_exp = var_exp_score(preds_smooth_mod, preds_smooth_gt)\n",
        "  mad_exp = mad_explained(preds_smooth_mod, preds_smooth_gt)\n",
        "  r_score = coef_det(preds_smooth_mod, preds_smooth_gt)\n",
        "\n",
        "  mse = torch.nn.MSELoss()(preds_smooth_mod, preds_smooth_gt)\n",
        "\n",
        "  return {\n",
        "      \"var_exp\": var_exp.item(),\n",
        "      \"mad_exp\": mad_exp.item(),\n",
        "      \"r_score\": r_score.item(),\n",
        "      \"mse\": mse.item()\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp9f7vtJeU83"
      },
      "outputs": [],
      "source": [
        "def score_model_interpolation(n_pairs = 1000, test_images = test_images, model = cnam_mlp_plus_bias):\n",
        "  res_lis = []\n",
        "  for i in tqdm(list(range(n_pairs))):\n",
        "    rnd_idx1 = torch.randint(len(test_images), size = (1,1))[0,0].item()\n",
        "    rnd_idx2 = torch.randint(len(test_images), size = (1,1))[0,0].item()\n",
        "\n",
        "    img1 = test_images[rnd_idx1]\n",
        "    img2 = test_images[rnd_idx2]\n",
        "\n",
        "    res = score_prediction_ground_truth(img1, img2, prediction_model = model)\n",
        "    res[\"idx1\"] = rnd_idx1\n",
        "    res[\"idx2\"] = rnd_idx2\n",
        "\n",
        "    res_lis.append(res)\n",
        "\n",
        "  import pandas as pd\n",
        "  res_df = pd.DataFrame(res_lis)\n",
        "\n",
        "  display(res_df.describe())\n",
        "\n",
        "  return res_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "18e8222ae19a4daf8611ff0b71e857de",
            "8233a3f706ed4e86ba8283d8a24708b4",
            "0c25160e00bf45adbd5e71ce1a010e20",
            "127c34d111044ecdb4a69fbd8d580a0a",
            "ed0bd2d3e5f5414b890ff53aac607b81",
            "13937cd051c544c2b39a5b2c1b529509",
            "2beeac53f6f6438aadf1612d5feb92fc",
            "63720d087e5d4809b807db2e6a035d07",
            "60a342d9a9dc43929b4fdecf3347427e",
            "617d3fb9979948c68a764a449d2aae81",
            "6cb94c5ccfea47a19e18df5a08af5f97"
          ]
        },
        "id": "z8IlzeJXekLG",
        "outputId": "9fe1f38a-fb4a-4d8d-b2d2-e0be5acdc8ab"
      },
      "outputs": [],
      "source": [
        "res_int_socres = score_model_interpolation(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "xCWCI1r5v8DC",
        "outputId": "56b49e8f-87c7-45b3-a7cb-05776680ca48"
      },
      "outputs": [],
      "source": [
        "res_int_socres.replace([np.inf, -np.inf], np.nan, inplace=False).dropna().describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6ULPASPJnqy"
      },
      "source": [
        "## Manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GI-xCFXLAqp"
      },
      "source": [
        "To obtain the manipulation vectors, a linear svm is fit on data comprising the semantic encodings of all the images and the known nearest quadrants of the white squares.\n",
        "The normal vector to interpolate between images with the square in quadrant $q_1$ and quadrant $q_2$ is obtained by training a SVM differntiating between samples where the square is nearest to the center of $q_1$ or $q_2$\n",
        "and extracting the normal vector of the separating hyperplane."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_PVUZhBJpj1"
      },
      "source": [
        "Images ore manipulated by taking an image where the white square is exclusively in one of the quadrants, followed by encoding the image with a stochastic and a semantic subcode.\n",
        "Then, the manipulation vector is added with increasing scale to the semantic subcode yielding the latent interpolation path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96TLxkJdYajA"
      },
      "outputs": [],
      "source": [
        "save_path_man = f\"{result_path}/Manipulation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTJS8N5yYQF9"
      },
      "outputs": [],
      "source": [
        "from templates import *\n",
        "\n",
        "def manipulate_quadrants(q1, q2, prediction_model, images = images, T_encoding = 2, T_decoding = 250, n_datapoints = 1000, n_images = 10, alpha_max = 1, normal_vec_dict = normal_vec_dict):\n",
        "  \"\"\"\n",
        "  Interpolates between images where the white square is in quadrant q1 and quadrant q2.\n",
        "  params:\n",
        "    q1: quadrant for first image\n",
        "    q2: quadrant for last image\n",
        "    T_encoding: number of steps for the stochastic encoder\n",
        "    T_decoding: number of steps for the stochastic decoder\n",
        "    alpha_max: maximal value of alpha until which is manipulated\n",
        "  \"\"\"\n",
        "\n",
        "  #img1 = images[:999][labels[:999] == q1][0]\n",
        "  img1 = images[4]\n",
        "\n",
        "  normal_vector = torch.tensor(normal_vec_dict[(q1, q2)])[0]\n",
        "\n",
        "  batch = torch.stack([\n",
        "    img1,\n",
        "  ]).float()\n",
        "\n",
        "  cond = model.encode(batch.to(device))\n",
        "  xT = model.encode_stochastic(batch.to(device), cond, T=T_encoding)\n",
        "\n",
        "  alpha_range_img = np.linspace(0, alpha_max, n_images, dtype=np.float32)\n",
        "  intp = torch.cat([cond +alpha* (torch.norm(cond)/torch.norm(normal_vector))*normal_vector.to(device) for alpha in alpha_range_img])\n",
        "\n",
        "  xT_rep = xT.repeat(n_images, 1, 1, 1)\n",
        "\n",
        "  images = model.render(xT_rep.half(), intp.half(), T=T_decoding) ##\n",
        "\n",
        "  preds_images = prediction_model(intp.float())[:, 0].detach().cpu().numpy()\n",
        "\n",
        "\n",
        "  alpha_detailed = (np.linspace(0, alpha_max, n_datapoints, dtype=np.float32))\n",
        "  intp_detailed = torch.cat([cond +alpha* (torch.norm(cond)/torch.norm(normal_vector))*normal_vector.to(device) for alpha in alpha_detailed])\n",
        "\n",
        "\n",
        "  preds_detailed = prediction_model(intp.float())[:, 0].detach().cpu().numpy()\n",
        "\n",
        "  plot_images_and_predictions(images, preds_images,preds_detailed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_KZV79mAUhN"
      },
      "outputs": [],
      "source": [
        "def create_manipulated_image_path(img, normal_vec, n_images = 10, alpha_max = 1.0, T_encoding = 2, T_decoding = 200):\n",
        "\n",
        "  batch = torch.stack([\n",
        "    img,\n",
        "  ]).float()\n",
        "\n",
        "  cond = model.encode(batch.to(device))\n",
        "  xT = model.encode_stochastic(batch.to(device), cond, T=T_encoding)\n",
        "\n",
        "  alpha_range_img = np.linspace(0, alpha_max, n_images, dtype=np.float32)\n",
        "  intp = torch.cat([cond +alpha* (torch.norm(cond)/torch.norm(normal_vec))*normal_vec.to(device) for alpha in alpha_range_img])\n",
        "\n",
        "  xT_rep = xT.repeat(n_images, 1, 1, 1)\n",
        "\n",
        "  images = model.render(xT_rep.half(), intp.half(), T=T_decoding)\n",
        "\n",
        "  images = torch.stack([img for img in images])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return images, cond, intp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6ZmMAoxACUG"
      },
      "outputs": [],
      "source": [
        "def manipulate_images(img1,\n",
        "                      normal_vec,\n",
        "                      prediction_model,\n",
        "                      alpha_max = 1.0,\n",
        "                      feature_extraction_fun = average_red_value_from_tensor,\n",
        "                      prediction_fun = f_img,\n",
        "                      images = images,\n",
        "                      n_datapoints = 1000,\n",
        "                      n_images= 10,\n",
        "                      T_encoding = 2,\n",
        "                      T_decoding = 200,\n",
        "                      ymin=-1.3,\n",
        "                      ymax=1.3,\n",
        "                      save_path = None):\n",
        "\n",
        "\n",
        "  generated_images, cond, intp = create_manipulated_image_path(img1, normal_vec, n_images, alpha_max, T_encoding, T_decoding)\n",
        "\n",
        "  preds_images = prediction_model(intp.float())[:, 0].detach().cpu().numpy()\n",
        "\n",
        "\n",
        "  alpha_detailed = (np.linspace(0, alpha_max, n_datapoints, dtype=np.float32))\n",
        "  intp_detailed = torch.cat([cond +alpha* (torch.norm(cond)/torch.norm(normal_vec))*normal_vec.to(device) for alpha in alpha_detailed])\n",
        "\n",
        "  preds_detailed = prediction_model(intp_detailed.float())[:, 0].detach().cpu().numpy()\n",
        "  mean = np.mean(preds_detailed)\n",
        "  preds_detailed = preds_detailed - mean\n",
        "  preds_images = preds_images - mean\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  generated_images = torch.stack([img*2-1 for img in generated_images])    #####CHANGED!!!\n",
        "\n",
        "  generated_images = (generated_images - generated_images.min())/(generated_images.max()- generated_images.min())\n",
        "\n",
        "\n",
        "  #generated_images[0] = img1\n",
        "  feats_gt = [feature_extraction_fun(img) for img in generated_images]\n",
        "  preds_images_gt = np.array([prediction_fun(feat) for feat in feats_gt])\n",
        "\n",
        "  #print(preds_images_gt)\n",
        "\n",
        "  preds_images_gt = preds_images_gt - np.mean(preds_images_gt)\n",
        "\n",
        "\n",
        "  plot_images_and_predictions2(images1 = generated_images,\n",
        "                               image_preds1 = preds_images_gt,\n",
        "                               preds_smooth1 = None,\n",
        "                               save_path = save_path,\n",
        "                               images2 = None,\n",
        "                               image_preds2= preds_images,  #preds_images\n",
        "                               preds_smooth2 = preds_detailed,\n",
        "                               dpi_save = 400,\n",
        "                               ymin=-1.3,\n",
        "                               ymax=1.3)\n",
        "\n",
        "  print(preds_images)\n",
        "  return generated_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reWgyVM8Xiz2",
        "outputId": "486c4cd9-8d95-4cc5-a290-9896f74b4a07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.5878)"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f_img(0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrBtx7QBpRdO"
      },
      "outputs": [],
      "source": [
        "cnam_mlp_plus_bias = lambda x: cnam.mlp(x) + cnam.feat_nam.bias.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSR8dzF_aIvB"
      },
      "outputs": [],
      "source": [
        "save_path_man = f\"{result_path}/Manipulation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97ISoghcUuWh"
      },
      "outputs": [],
      "source": [
        "nv01 = normal_vec_dict[(0,1)]\n",
        "nv02 = normal_vec_dict[(0,2)]\n",
        "nv12 = normal_vec_dict[(1,2)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "y3NJjzb7VRY5",
        "outputId": "a127a54b-6f88-4465-e9e3-9e7f4a54a817"
      },
      "outputs": [],
      "source": [
        "img1 = generate_img(0.5, g = 0.1, b = 0.2)\n",
        "\n",
        "#img1 = generate_img(0.2, g = 0.5, b = 0)\n",
        "#img1 = images[5].cpu()\n",
        "\n",
        "plt.imshow(img1.permute(1,2,0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU9SidreaHfV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "id": "XF64bfIEVcp1",
        "outputId": "8a69fa81-93c4-4ce4-ad3d-df536eadcf64"
      },
      "outputs": [],
      "source": [
        "images_man = manipulate_images(img1, torch.tensor(nv01), cnam_mlp_plus_bias, alpha_max = 0.3, T_encoding = 250, T_decoding = 250, n_images = 10, save_path = save_path_man)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "id": "RNHU_JYqBl3M",
        "outputId": "9ac43522-e07b-4c0b-8dcd-3ee73597b55b"
      },
      "outputs": [],
      "source": [
        "images_man = manipulate_images(img1, torch.tensor(nv02), cnam_mlp_plus_bias, alpha_max = 0.3, T_encoding = 250, T_decoding = 250, n_images = 10, save_path = save_path_man)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "vd9CW-YwB8pS",
        "outputId": "a6ff6d65-11dd-40f9-bb48-5ff0d2b16180"
      },
      "outputs": [],
      "source": [
        "img2 = generate_img(0.1, g = 0.5, b = 0.2)\n",
        "\n",
        "#img1 = generate_img(0.2, g = 0.5, b = 0)\n",
        "#img1 = images[5].cpu()\n",
        "\n",
        "plt.imshow(img2.permute(1,2,0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "id": "DtRp_S8_CDn3",
        "outputId": "296f59c4-da48-4da0-b2cc-d9b67b6f377a"
      },
      "outputs": [],
      "source": [
        "images_man = manipulate_images(img2, torch.tensor(nv12), cnam_mlp_plus_bias, alpha_max = 0.3, T_encoding = 250, T_decoding = 250, n_images = 10, save_path = save_path_man)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvZEQRCBCMOu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c25160e00bf45adbd5e71ce1a010e20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63720d087e5d4809b807db2e6a035d07",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60a342d9a9dc43929b4fdecf3347427e",
            "value": 1000
          }
        },
        "127c34d111044ecdb4a69fbd8d580a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_617d3fb9979948c68a764a449d2aae81",
            "placeholder": "",
            "style": "IPY_MODEL_6cb94c5ccfea47a19e18df5a08af5f97",
            "value": " 1000/1000 [03:04&lt;00:00,  5.48it/s]"
          }
        },
        "13937cd051c544c2b39a5b2c1b529509": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18e8222ae19a4daf8611ff0b71e857de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8233a3f706ed4e86ba8283d8a24708b4",
              "IPY_MODEL_0c25160e00bf45adbd5e71ce1a010e20",
              "IPY_MODEL_127c34d111044ecdb4a69fbd8d580a0a"
            ],
            "layout": "IPY_MODEL_ed0bd2d3e5f5414b890ff53aac607b81"
          }
        },
        "2beeac53f6f6438aadf1612d5feb92fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60a342d9a9dc43929b4fdecf3347427e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "617d3fb9979948c68a764a449d2aae81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63720d087e5d4809b807db2e6a035d07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cb94c5ccfea47a19e18df5a08af5f97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8233a3f706ed4e86ba8283d8a24708b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13937cd051c544c2b39a5b2c1b529509",
            "placeholder": "",
            "style": "IPY_MODEL_2beeac53f6f6438aadf1612d5feb92fc",
            "value": "100%"
          }
        },
        "ed0bd2d3e5f5414b890ff53aac607b81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
