{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0yDjpd_1MME",
        "outputId": "55d4213d-b412-48de-ee2e-5e41909db3bc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "67YNeOVx_qBC",
        "outputId": "4f512547-a2b1-4d0a-dca4-a77b48139f39"
      },
      "outputs": [],
      "source": [
        "#@markdown <h3> Copied code for setup </h3>\n",
        "#!python -m pip install torch==1.11.0+cu102 torchvision==0.12.0+cu102 pytorch-lightning  torchtext -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 pytorch-lightning --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "!python -m  install scipy==1.5.4\n",
        "!python -m  install numpy==1.19.5\n",
        "!python -m  install tqdm\n",
        "!python -m  install pytorch-fid==0.2.0\n",
        "!python -m  install pandas==1.1.5\n",
        "!python -m  install lpips==0.1.4\n",
        "!python -m  install lmdb==1.2.1\n",
        "!python -m  install ftfy\n",
        "!python -m  install regex\n",
        "!python -m  install dlib requests\n",
        "\n",
        "\n",
        "!git clone https://github.com/phizaz/diffae\n",
        "!git pull https://github.com/phizaz/diffae\n",
        "\n",
        "\n",
        "\n",
        "%cd 'diffae'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUaii4krA4lM"
      },
      "outputs": [],
      "source": [
        "#!pip install torchtext==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0tCoqxlT_Cr",
        "outputId": "e47095fe-550b-4c6d-8ca2-0085fec8b50e"
      },
      "outputs": [],
      "source": [
        "!pip install pip install lmdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpYt9yjVUQ_e",
        "outputId": "2dd15288-e4d8-46f2-ac24-852b5fb59008"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch_fid\n",
        "!pip install lpips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlNhatUmNTQE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "#torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A4nVNkUAAIG",
        "outputId": "ff9f4248-f61c-47cf-8914-233f103090e3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')#@markdown <h3> ðŸ”§ Import libraries and tools </h3>\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from templates import *\n",
        "from templates_cls import *\n",
        "from experiment_classifier import ClsModel\n",
        "\n",
        "def show_images(images, cols = 1, titles = None, apply_convert=False):\n",
        "    if apply_convert: images = [convert(img) for img in images]\n",
        "    assert((titles is None)or (len(images) == len(titles)))\n",
        "    n_images = len(images)\n",
        "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
        "    fig = plt.figure()\n",
        "    for n, (image, title) in enumerate(zip(images, titles)):\n",
        "        a = fig.add_subplot(cols, int(np.ceil(n_images/float(cols))), n + 1)\n",
        "        if image.ndim == 2:\n",
        "            plt.gray()\n",
        "        plt.imshow(image)\n",
        "        a.set_title(title)\n",
        "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images*2)\n",
        "    plt.show()\n",
        "\n",
        "def convert2rgb(img,adjust_scale=True):\n",
        "    convert_img = torch.tensor(img)\n",
        "    if adjust_scale: convert_img = (convert_img+1)/2\n",
        "    return (convert_img).permute(1, 2, 0).cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JGNd9u3AJ-y",
        "outputId": "71ed3d3d-654c-4799-c947-a59d43c1f669"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "conf = ffhq256_autoenc()\n",
        "# print(conf.name)\n",
        "pretrained_encoder = LitModel(conf)\n",
        "state = torch.load(\"\", map_location='cpu')\n",
        "pretrained_encoder.load_state_dict(state['state_dict'], strict=False)\n",
        "pretrained_encoder.ema_model.eval()\n",
        "pretrained_encoder.ema_model.to(device);\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4zZQUTItnpX"
      },
      "source": [
        "create semantic subcodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWiLqbF6dZ1l"
      },
      "outputs": [],
      "source": [
        "img_loc = \"\"\n",
        "images = torch.load(img_loc, map_location = \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A3Ceka3mtg_",
        "outputId": "87cd665e-f1f8-4425-a1f0-44dce4690ed8"
      },
      "outputs": [],
      "source": [
        "len(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWLLIIbxxGUI"
      },
      "outputs": [],
      "source": [
        "df_loc = \"\"\n",
        "\n",
        "target_df = pd.read_excel(df_loc)\n",
        "\n",
        "#target_df.drop(2015, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P29e6Kn4AjT",
        "outputId": "2ca9d1cf-dcf1-4f40-fa4a-05cc6dfdc024"
      },
      "outputs": [],
      "source": [
        "len(target_df), len(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "qVbp2C1O1aue",
        "outputId": "b2fad841-6c90-4ea8-e46b-a34d508abee8"
      },
      "outputs": [],
      "source": [
        "res_lis = []\n",
        "valid_idx_lis = []\n",
        "\n",
        "for i, s in enumerate(target_df[\"price\"]):\n",
        "  try:\n",
        "    if type(s) == str:\n",
        "      s = s[1:]\n",
        "      s = s.replace(\",\", \"\")\n",
        "      res_lis.append(float(s))\n",
        "      valid_idx_lis.append(i)\n",
        "  except:\n",
        "    print(s)\n",
        "\n",
        "targets = torch.tensor(res_lis)\n",
        "images = images[valid_idx_lis]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFfCfDeA2k_P"
      },
      "outputs": [],
      "source": [
        "minimum = torch.quantile(targets, 0.05)\n",
        "maximum = torch.quantile(targets, 1 - 0.05)\n",
        "valid_idx2 = torch.logical_and(targets > minimum, targets < maximum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "0wd51lmz407G",
        "outputId": "c90a4530-f594-4e88-bc11-0d73385c0cf7"
      },
      "outputs": [],
      "source": [
        "targets = targets[valid_idx2]\n",
        "images = images[valid_idx2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "CncMifLi6LZ8",
        "outputId": "527d5361-547d-4081-abe0-6ca56855aea6"
      },
      "outputs": [],
      "source": [
        "targets = torch.log(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "1LXNfv2u1kwh",
        "outputId": "6f533acd-13bc-4cc1-c8a6-c2d7b97a86a2"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.hist(targets, bins = 100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av41DmTrx0E-"
      },
      "source": [
        "## Preprocess DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "h93mmZSux2ZA",
        "outputId": "fbe34306-ce60-4239-8caf-149a626854a5"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "\n",
        "\n",
        "imputer = imputer.fit(\n",
        "    target_df[\n",
        "        [\n",
        "            \"accommodates\",\n",
        "            \"bedrooms\",\n",
        "            \"number_of_reviews\",\n",
        "            \"review_scores_value\",\n",
        "            \"minimum_nights\",\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "\n",
        "target_df[\n",
        "    [\n",
        "        \"accommodates\",\n",
        "        \"bedrooms\",\n",
        "        \"number_of_reviews\",\n",
        "        \"review_scores_value\",\n",
        "        \"minimum_nights\",\n",
        "    ]\n",
        "] = imputer.transform(\n",
        "    target_df[\n",
        "        [\n",
        "            \"accommodates\",\n",
        "            \"bedrooms\",\n",
        "            \"number_of_reviews\",\n",
        "            \"review_scores_value\",\n",
        "            \"minimum_nights\",\n",
        "        ]\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTo75UhyyAaj"
      },
      "outputs": [],
      "source": [
        "target_df = target_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "wfwCwVigz7Ud",
        "outputId": "9c05fe27-fd21-4a61-ea4d-7ad63aaa058a"
      },
      "outputs": [],
      "source": [
        "target_df = target_df[valid_idx2.numpy()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "7HQNFMCiyVuo",
        "outputId": "8eeca940-577b-4b04-9dd3-b8c922b4a391"
      },
      "outputs": [],
      "source": [
        "train_df = target_df.reset_index(drop=True)\n",
        "train_df = train_df[\n",
        "    [\n",
        "        #\"host_is_superhost\",\n",
        "        #\"latitude\",\n",
        "        #\"longitude\",\n",
        "        \"room_type\",\n",
        "        \"accommodates\",\n",
        "        \"bedrooms\",\n",
        "        \"minimum_nights\",\n",
        "        \"number_of_reviews\",\n",
        "        \"review_scores_value\",\n",
        "        \"host_identity_verified\",\n",
        "    ]\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "aR0PVGIMwVVz",
        "outputId": "47c24cef-cf23-4912-91ce-be48c1ff5cd3"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "train_df_unscaled = deepcopy(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIbHn-nbyaNG"
      },
      "outputs": [],
      "source": [
        "\n",
        "def normalize_df(df):\n",
        "  df = (df - df.mean())/df.std()\n",
        "  return df\n",
        "\n",
        "def min_max_df(df):\n",
        "  df = min_max_scaler.fit_transform(df)\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "def transform_data2(df):\n",
        "  num_cols = [dt.kind != \"O\" for dt in df.dtypes]\n",
        "\n",
        "  df[df.columns.values[num_cols]] = normalize_df(df[df.columns.values[num_cols]])\n",
        "  df = pd.get_dummies(df)\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELGwzSah4HD9"
      },
      "outputs": [],
      "source": [
        "train_df_unscaled = pd.get_dummies(train_df_unscaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfmHV7aGyfK3"
      },
      "outputs": [],
      "source": [
        "train_df = transform_data2(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0tQk2mDyhsc"
      },
      "outputs": [],
      "source": [
        "features = np.array(train_df)\n",
        "col_names = list(train_df.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhGJnz1ZyrZc"
      },
      "source": [
        "## Define NAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "ExWHOo9Jyvmk",
        "outputId": "3858bb07-f0fd-431b-8bb7-3139ee67d278"
      },
      "outputs": [],
      "source": [
        "class FeatureNN(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 shallow_units: int,   # number of neurons in first layer\n",
        "                 hidden_units = [],  # tuple of numbers of hidden units\n",
        "                 activation = torch.nn.ReLU(),\n",
        "                 dropout: float = .5,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define Layers\n",
        "        self.layers = torch.nn.ModuleList([\n",
        "            torch.nn.Linear(shallow_units if i == 0 else hidden_units[i - 1], hidden_units[i])\n",
        "            for i in range(len(hidden_units))\n",
        "        ])\n",
        "\n",
        "        self.layers.insert(0, torch.nn.Linear(1, shallow_units))\n",
        "        self.output_layer = torch.nn.Linear(hidden_units[-1], 1)\n",
        "\n",
        "        # Dropout and activation\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "        self.activation = activation\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            x = self.activation(x)\n",
        "            x = self.dropout(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class NAM(torch.nn.Module):\n",
        "  def __init__(self,\n",
        "                n_features,\n",
        "                shallow_units: int,   # number of neurons in first layer\n",
        "                hidden_units = [],  # tuple of numbers of hidden units\n",
        "                activation = torch.nn.ReLU(),\n",
        "                dropout: float = .5,\n",
        "                feature_dropout = 0.0,\n",
        "                return_output_lis = False\n",
        "                ):\n",
        "      super().__init__()\n",
        "\n",
        "      self.shallow_units = shallow_units\n",
        "      self.hidden_units = hidden_units\n",
        "      self.activation = activation\n",
        "      self.dropout = dropout\n",
        "\n",
        "      self.n_features = n_features\n",
        "      self.feature_dropout_rate = feature_dropout\n",
        "      self.return_output_lis = return_output_lis\n",
        "\n",
        "      self.feature_nns = torch.nn.ModuleList([\n",
        "            FeatureNN(shallow_units=shallow_units,\n",
        "                      hidden_units=hidden_units,\n",
        "                      activation=activation,\n",
        "                      dropout=dropout)\n",
        "            for i in range(n_features)\n",
        "        ])\n",
        "\n",
        "      self.bias = torch.nn.Parameter(torch.zeros(1))\n",
        "      self.feature_dropout = torch.nn.Dropout(p=self.feature_dropout_rate)\n",
        "\n",
        "  def forward(self, x, f):\n",
        "    eta = self.bias\n",
        "    output_lis = []\n",
        "    for feature, mod in zip(f.T, self.feature_nns):\n",
        "      feature = feature.unsqueeze(-1)\n",
        "      ri = mod(feature)\n",
        "      output_lis.append(ri)\n",
        "\n",
        "    if self.return_output_lis:\n",
        "      return output_lis\n",
        "\n",
        "    else:\n",
        "       conc_out = torch.cat(output_lis, dim=-1)\n",
        "       dropout_out = self.feature_dropout(conc_out)\n",
        "       out = torch.sum(dropout_out, dim=-1) + self.bias\n",
        "\n",
        "       return out\n",
        "\n",
        "\n",
        "class NAM_plus_CNN(torch.nn.Module):\n",
        "  def __init__(self,\n",
        "                pretrained_encoder,\n",
        "                mlp,\n",
        "                feat_nam,\n",
        "                ):\n",
        "      super().__init__()\n",
        "\n",
        "      self.pretrained_encoder = pretrained_encoder\n",
        "      self.mlp = mlp\n",
        "      self.feat_nam = feat_nam\n",
        "      self.feat_nam.return_output_lis = True\n",
        "\n",
        "  def forward(self, image, features):\n",
        "    output_lis_nam = self.feat_nam(_, features)\n",
        "\n",
        "    res_cnn = self.pretrained_encoder.encode(image)\n",
        "    res_cnn = self.mlp(res_cnn)\n",
        "\n",
        "    output_lis = output_lis_nam + [res_cnn]\n",
        "    output_lis = torch.cat(output_lis, dim=-1)\n",
        "\n",
        "\n",
        "    output_lis = self.feat_nam.feature_dropout(output_lis)\n",
        "    out = torch.sum(output_lis, dim=-1) + self.feat_nam.bias\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BXL2GPKwrmy",
        "outputId": "ee3910b5-6c23-45ac-8fa5-81ec454cf172"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([256, 256])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "img_shape = images[0][0].shape\n",
        "img_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSBUehULxIE9"
      },
      "outputs": [],
      "source": [
        "#mean = torch.mean(images, dim = 0)\n",
        "#std = torch.std(images, dim = 0)\n",
        "\n",
        "image_size = conf.img_size\n",
        "\n",
        "transforms_train = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.RandomApply([torchvision.transforms.ColorJitter(brightness = (0.8, 1.3))], p=0.5),\n",
        "    torchvision.transforms.RandomApply([torchvision.transforms.ColorJitter(contrast = (0.8, 1.3))], p=0.5),\n",
        "    #torchvision.transforms.RandomApply([torchvision.transforms.ColorJitter(saturation = (0.5, 1.5))], p=0.5),   # Skin color?\n",
        "    #torchvision.transforms.RandomApply([torchvision.transforms.Grayscale()], p = 0.1),\n",
        "    torchvision.transforms.RandomApply([torchvision.transforms.GaussianBlur(kernel_size = 11)], p = 0.2),\n",
        "    torchvision.transforms.Resize(image_size),\n",
        "    torchvision.transforms.CenterCrop(image_size)\n",
        "    #torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transforms_val_test = torchvision.transforms.Compose([\n",
        "     torchvision.transforms.Resize(image_size),\n",
        "     torchvision.transforms.CenterCrop(image_size)\n",
        "     #torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG8vDvnB2pfh"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "    def __init__(self, img_ten, label_ten, transforms=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.img_ten = img_ten\n",
        "        self.label_ten = label_ten\n",
        "        self.transforms = transforms\n",
        "\n",
        "        assert len(img_ten) == len(label_ten), \"img_ten and label ten must have equal size\"\n",
        "    def __len__(self):\n",
        "      return len(self.img_ten)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      x = self.img_ten[idx]\n",
        "      y = self.label_ten[idx]\n",
        "\n",
        "      if not self.transforms == None:\n",
        "        x = self.transforms(x)\n",
        "\n",
        "      return x, y\n",
        "\n",
        "    def set_transforms(self, transforms):\n",
        "      self.transforms = transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "-fnLwklXAtKK",
        "outputId": "9e1582d3-065b-4318-86f8-f00931c81165"
      },
      "outputs": [],
      "source": [
        "class CustomDataset_img_features(torch.utils.data.Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "    def __init__(self, img_ten, feature_ten, label_ten, transforms=None):\n",
        "\n",
        "        self.img_ten = img_ten\n",
        "        self.label_ten = label_ten\n",
        "        self.feature_ten = feature_ten\n",
        "        self.transforms = transforms\n",
        "\n",
        "        assert len(img_ten) == len(label_ten) ==  len(feature_ten), \"img_ten and and feature_ten and label ten must have equal size\"\n",
        "    def __len__(self):\n",
        "      return len(self.img_ten)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      x = self.img_ten[idx]\n",
        "      feat = self.feature_ten[idx]\n",
        "      y = self.label_ten[idx]\n",
        "\n",
        "      if not self.transforms == None:\n",
        "        x = self.transforms(x)\n",
        "\n",
        "      return x, feat, y\n",
        "\n",
        "    def set_transforms(self, transforms):\n",
        "      self.transforms = transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s7fVYemL-XXk"
      },
      "outputs": [],
      "source": [
        "def train_test_split_images(images, targets, train_frac, val_frac, batch_size, transforms_train, transforms_val_test):\n",
        "    tot_len = len(images)\n",
        "\n",
        "    train_max_idx = int(tot_len*train_frac)\n",
        "    val_max_idx = int(tot_len*val_frac) + train_max_idx\n",
        "\n",
        "    train_img = images[:train_max_idx]\n",
        "    train_y = targets[:train_max_idx]\n",
        "\n",
        "    val_img = images[train_max_idx:val_max_idx]\n",
        "    val_y = targets[train_max_idx:val_max_idx]\n",
        "\n",
        "    test_img = images[val_max_idx:]\n",
        "    test_y = targets[val_max_idx:]\n",
        "\n",
        "    train_set = CustomDataset(train_img, train_y, transforms = transforms_train)\n",
        "    val_set = CustomDataset(val_img, val_y, transforms = transforms_val_test)\n",
        "    test_set = CustomDataset(test_img, test_y, transforms = transforms_val_test)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_set, batch_size = batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "def train_test_split_features(features, targets, train_frac, val_frac, batch_size):\n",
        "    tot_len = len(features)\n",
        "\n",
        "    train_max_idx = int(tot_len*train_frac)\n",
        "    val_max_idx = int(tot_len*val_frac) + train_max_idx\n",
        "\n",
        "    train_features = features[:train_max_idx]\n",
        "    train_y = targets[:train_max_idx]\n",
        "\n",
        "    val_features = features[train_max_idx:val_max_idx]\n",
        "    val_y = targets[train_max_idx:val_max_idx]\n",
        "\n",
        "    test_features = features[val_max_idx:]\n",
        "    test_y = targets[val_max_idx:]\n",
        "\n",
        "    train_set = list(zip(train_features, train_y))\n",
        "    val_set = list(zip(val_features, val_y))\n",
        "    test_set = list(zip(test_features, test_y))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_set, batch_size = batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "def train_test_split_features_images(features, images, targets, train_frac, val_frac, batch_size, transforms_train, transforms_val_test):\n",
        "    tot_len = len(features)\n",
        "\n",
        "    train_max_idx = int(tot_len*train_frac)\n",
        "    val_max_idx = int(tot_len*val_frac) + train_max_idx\n",
        "\n",
        "    train_features = features[:train_max_idx]\n",
        "    train_images = images[:train_max_idx]\n",
        "    train_y = targets[:train_max_idx]\n",
        "\n",
        "    val_features = features[train_max_idx:val_max_idx]\n",
        "    val_images = images[train_max_idx:val_max_idx]\n",
        "    val_y = targets[train_max_idx:val_max_idx]\n",
        "\n",
        "    test_features = features[val_max_idx:]\n",
        "    test_images = images[val_max_idx:]\n",
        "    test_y = targets[val_max_idx:]\n",
        "\n",
        "    train_set = CustomDataset_img_features(train_images, train_features, train_y, transforms = transforms_train)\n",
        "    val_set = CustomDataset_img_features(val_images, val_features, val_y, transforms = transforms_val_test)\n",
        "    test_set = CustomDataset_img_features(test_images, test_features, test_y, transforms = transforms_val_test)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_set, batch_size = batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7c5DtVnElzt"
      },
      "outputs": [],
      "source": [
        "y_mean = torch.mean(targets)\n",
        "y_std = torch.std(targets)\n",
        "\n",
        "standardize_targets = lambda x: (x - y_mean)/y_std\n",
        "unstandardize_targets = lambda x: x*y_std + y_mean\n",
        "unstandardize_and_exp_targets = lambda x: torch.exp(x*y_std + y_mean)\n",
        "\n",
        "targets_standardized = (targets - y_mean)/y_std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBzRC2rDlOqM",
        "outputId": "08140d5e-7e8f-4f66-ace3-58802c252c19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-38-9772b75213fe>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets = torch.tensor(targets_standardized).float(),\n"
          ]
        }
      ],
      "source": [
        "train_loader, val_loader, test_loader = train_test_split_features_images(images = images,\n",
        "                                                         features = torch.tensor(features).float(),\n",
        "                                                         targets = torch.tensor(targets_standardized).float(),\n",
        "                                                         train_frac = 0.7,\n",
        "                                                         val_frac = 0.2,\n",
        "                                                         batch_size = 128,\n",
        "                                                         transforms_train= transforms_train,\n",
        "                                                         transforms_val_test = transforms_val_test)   # create dataloader with embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiKERfDwwKQY"
      },
      "outputs": [],
      "source": [
        "class One_Layer_MLP(torch.nn.Module):\n",
        "  def __init__(self,  n_input_units):\n",
        "\n",
        "    super(One_Layer_MLP, self).__init__()\n",
        "\n",
        "    self.n_input_units = n_input_units\n",
        "    self.fc1 = torch.nn.Linear(n_input_units, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Linear_skip_block(nn.Module):\n",
        "  \"\"\"\n",
        "  Block of linear layer + softplus + skip connection +  dropout  + batchnorm\n",
        "  \"\"\"\n",
        "  def __init__(self, n_input, dropout_rate):\n",
        "    super(Linear_skip_block, self).__init__()\n",
        "\n",
        "    self.fc = nn.Linear(n_input, n_input)\n",
        "    self.act = torch.nn.LeakyReLU()\n",
        "\n",
        "    self.bn = nn.BatchNorm1d(n_input, affine = True)\n",
        "    self.drop = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x0 = x\n",
        "    x = self.fc(x)\n",
        "    x = self.act(x)\n",
        "    x = x0 + x\n",
        "    x = self.drop(x)\n",
        "    x = self.bn(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Linear_block(nn.Module):\n",
        "  \"\"\"\n",
        "  Block of linear layer dropout  + batchnorm\n",
        "  \"\"\"\n",
        "  def __init__(self, n_input, n_output, dropout_rate):\n",
        "    super(Linear_block, self).__init__()\n",
        "\n",
        "    self.fc = nn.Linear(n_input, n_output)\n",
        "    self.act = torch.nn.LeakyReLU()\n",
        "    self.bn = nn.BatchNorm1d(n_output, affine = True)\n",
        "    self.drop = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc(x)\n",
        "    x = self.act(x)\n",
        "    x = self.drop(x)\n",
        "    x = self.bn(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, n_input_units, n_hidden_units, n_skip_layers, dropout_rate):\n",
        "\n",
        "    super(MLP, self).__init__()\n",
        "    self.n_input_units = n_input_units\n",
        "    self.n_hidden_units = n_hidden_units\n",
        "    self.n_skip_layers = n_skip_layers\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "    self.linear1 = Linear_block(n_input_units, n_hidden_units, dropout_rate)    # initial linear layer\n",
        "    self.hidden_layers = torch.nn.Sequential(*[Linear_skip_block(n_hidden_units, dropout_rate) for _ in range(n_skip_layers)])  #hidden skip-layers\n",
        "\n",
        "    self.linear_final =  torch.nn.Linear(n_hidden_units, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.hidden_layers(x)\n",
        "    x = self.linear_final(x)\n",
        "\n",
        "    return(x)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self, pretrained_model, mlp):\n",
        "    super(CNN, self).__init__()\n",
        "\n",
        "    self.pretrained_model = pretrained_model\n",
        "    self.pretrained_model.requires_grad = False\n",
        "    self.mlp = mlp\n",
        "\n",
        "  def forward(self, x, f):\n",
        "    with torch.no_grad():\n",
        "      x = self.pretrained_model.encode(x)\n",
        "    x = self.mlp(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-PFJ_MKDgdx"
      },
      "outputs": [],
      "source": [
        "def var_exp_score(predictions, targets):\n",
        "  mean_sum_of_squares = torch.mean((predictions - targets)**2)\n",
        "  variance_targets = torch.var(targets)\n",
        "  var_exp = 1- mean_sum_of_squares/variance_targets\n",
        "\n",
        "  return var_exp\n",
        "\n",
        "def coef_det(x, y):\n",
        "  sum_x = torch.sum(x)\n",
        "  sum_y = torch.sum(y)\n",
        "  n = len(x)\n",
        "\n",
        "  numerator = n * torch.sum(x * y) - sum_x*sum_x\n",
        "  denominator = (n * torch.sum(x**2) - sum_x**2)**0.5 * (n * torch.sum(y**2) - sum_y**2)**0.5\n",
        "\n",
        "  return numerator/denominator\n",
        "\n",
        "\n",
        "def mad_explained(predictions, targets):\n",
        "  mean_sum_of_ad = torch.mean(torch.abs(predictions - targets))\n",
        "  deviation_median = torch.mean(torch.abs(targets - torch.median(targets)))\n",
        "\n",
        "  mad_exp = 1 - mean_sum_of_ad/deviation_median\n",
        "\n",
        "  return mad_exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_RUD6tZNDrW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "#Validation function\n",
        "\n",
        "def validate(model, dataloader, loss_fun):\n",
        "    val_loss_lis = []\n",
        "\n",
        "    target_lis = []\n",
        "    pred_lis = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "\n",
        "          x, f, y = batch\n",
        "          x = x.to(device)\n",
        "          f = f.to(device)\n",
        "          y = y.to(device)\n",
        "          pred = model(x, f)\n",
        "          pred = pred.squeeze(-1)\n",
        "          loss = loss_fun(pred, y)\n",
        "          val_loss_lis.append(loss.cpu().detach())\n",
        "\n",
        "          target_lis.append(y.detach().cpu())\n",
        "          pred_lis.append(pred.detach().cpu())\n",
        "\n",
        "    mean_loss = np.mean(np.array(val_loss_lis))\n",
        "    median_loss = np.median(np.array(val_loss_lis))\n",
        "\n",
        "    target_ten, pred_ten = torch.cat(target_lis), torch.cat(pred_lis)\n",
        "    var_exp = var_exp_score(pred_ten, target_ten)\n",
        "    mad_exp = mad_explained(pred_ten, target_ten)\n",
        "    r_score = coef_det(pred_ten, target_ten)\n",
        "    return mean_loss, median_loss, var_exp, mad_exp, r_score\n",
        "\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_loop(model, optimizer, loss_fun, trainset, valset, print_mod, device, n_epochs, save_path = None, early_stopping = True, n_epochs_early_stopping = 5):\n",
        "    \"\"\"\n",
        "    train the model\n",
        "    Args:\n",
        "        model: The model to train\n",
        "        optimizer: The used optimizer\n",
        "        loss_fun: The used loss function\n",
        "        trainset: The dataset to train on\n",
        "        valset: The dataset to use for validation\n",
        "        print_mod: Number of epochs to print result after\n",
        "        device: Either \"cpu\" or \"cuda\"\n",
        "        n_epochs: Number of epochs to train\n",
        "        save_path: Path to save the model's state dict\n",
        "        config: config file from the model to train\n",
        "        sparse_ten (bool): if a sparse tensor is used for each batch\n",
        "    \"\"\"\n",
        "    if early_stopping == True:\n",
        "      n_early_stopping = n_epochs_early_stopping\n",
        "      past_val_losses = []\n",
        "\n",
        "    loss_lis = []\n",
        "    target_lis = []\n",
        "    pred_lis = []\n",
        "\n",
        "    loss_lis_all = []\n",
        "    val_loss_lis_all = []\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(n_epochs):\n",
        "      start = time.time()\n",
        "      for iter, batch in enumerate(tqdm(trainset)):\n",
        "\n",
        "        x, f, y = batch\n",
        "        x = x.to(device)\n",
        "        f = f.to(device)\n",
        "        y = y.to(device)\n",
        "        pred = model(x, f)\n",
        "        pred = pred.squeeze(-1)\n",
        "\n",
        "\n",
        "        loss = loss_fun(pred, y)\n",
        "        #print(loss)\n",
        "\n",
        "        optimizer.zero_grad()       # clear previous gradients\n",
        "        loss.backward()             # backprop\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_lis.append(loss.cpu().detach())\n",
        "        target_lis.append(y.detach().cpu())\n",
        "        pred_lis.append(pred.detach().cpu())\n",
        "\n",
        "      if epoch % print_mod == 0:\n",
        "\n",
        "        end = time.time()\n",
        "        time_delta = end - start\n",
        "\n",
        "        mean_loss = np.mean(np.array(loss_lis))\n",
        "        median_loss = np.median(np.array(loss_lis))\n",
        "\n",
        "        target_ten, pred_ten = torch.cat(target_lis), torch.cat(pred_lis)\n",
        "        var_exp = var_exp_score(pred_ten, target_ten)\n",
        "        mad_exp = mad_explained(pred_ten, target_ten)\n",
        "        r_score = coef_det(pred_ten, target_ten)\n",
        "\n",
        "        target_lis = []\n",
        "        pred_lis = []\n",
        "\n",
        "\n",
        "\n",
        "        loss_lis_all += loss_lis\n",
        "\n",
        "        loss_lis = []\n",
        "\n",
        "\n",
        "\n",
        "        mean_loss_val, median_loss_val, var_exp_val, mad_exp_val, val_r_score = validate(model, val_loader, loss_fun = loss_fun)\n",
        "\n",
        "        val_loss_lis_all.append(mean_loss_val)\n",
        "\n",
        "\n",
        "\n",
        "        print(f'Epoch nr {epoch}: mean_train_loss = {mean_loss}, median_train_loss = {median_loss}, train_var_exp = {var_exp}, train_mad_exp = {mad_exp}, train_r = {r_score}, elapsed time: {time_delta}')\n",
        "        print(f'Epoch nr {epoch}: mean_valid_loss = {mean_loss_val}, median_valid_loss = {median_loss_val}, valid_var_exp = {var_exp_val}, valid_mad_exp = {mad_exp_val},  valid_r = {val_r_score}')\n",
        "\n",
        "\n",
        "\n",
        "        # early stopping based on median validation loss:\n",
        "        if early_stopping:\n",
        "          if len(past_val_losses) == 0 or mean_loss_val < min(past_val_losses):\n",
        "            print(\"save model\")\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "\n",
        "          if len(past_val_losses) >= n_early_stopping:\n",
        "            if mean_loss_val > max(past_val_losses):\n",
        "              print(f\"Early stopping because the median validation loss has not decreased since the last {n_early_stopping} epochs\")\n",
        "              return loss_lis_all, val_loss_lis_all\n",
        "            else:\n",
        "              past_val_losses = past_val_losses[1:] + [mean_loss_val]\n",
        "          else:\n",
        "            past_val_losses = past_val_losses + [mean_loss_val]\n",
        "\n",
        "\n",
        "\n",
        "    return loss_lis_all, val_loss_lis_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99aOY77n6BGQ"
      },
      "outputs": [],
      "source": [
        "from ast import Name\n",
        "\n",
        "\n",
        "save_path = \"\"\n",
        "\n",
        "mlp = MLP(n_input_units = 512,\n",
        "    n_hidden_units = 500,\n",
        "    n_skip_layers = 4,\n",
        "    dropout_rate = 0.5)\n",
        "\n",
        "#model_cnn = CNN(\n",
        "#    pretrained_model = pretrained_encoder,\n",
        "#    mlp = mlp\n",
        "\n",
        "#)\n",
        "\n",
        "\n",
        "nam = NAM(n_features=features.shape[-1],\n",
        "                            shallow_units=20,\n",
        "                            hidden_units=(100, 100, 100),\n",
        "                            activation= torch.nn.ReLU(),\n",
        "                            dropout=0.5,\n",
        "                            feature_dropout = 0.5\n",
        "          )\n",
        "model = NAM_plus_CNN(pretrained_encoder, mlp, nam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcznJPGlHuls"
      },
      "source": [
        "# Train MLP-head and NAM jointly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "rU5mTbC1RI3A",
        "outputId": "25c74d08-bc0c-4e61-c11f-d9563e9e32f0"
      },
      "outputs": [],
      "source": [
        "sdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "zCznWVfAB8C6",
        "outputId": "7d2562be-6f94-4bc7-d04c-99d7ccde74f2"
      },
      "outputs": [],
      "source": [
        "lr = 1e-3\n",
        "opt = torch.optim.AdamW(list(model.mlp.parameters()) + list(model.feat_nam.parameters()), lr = lr, weight_decay=1e-3)\n",
        "#opt = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFkfsRBfPA6c"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "train_loop(model = model,\n",
        "           optimizer = opt,\n",
        "           loss_fun = torch.nn.MSELoss(),     #torch.nn.L1Loss4\n",
        "           trainset = train_loader,\n",
        "           valset = val_loader,\n",
        "           print_mod = 1,\n",
        "           device = device,\n",
        "           early_stopping = True,\n",
        "           n_epochs_early_stopping = 100,\n",
        "           save_path = save_path,\n",
        "           n_epochs = 10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Usay8kJy1Ay"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "C5DfTValAW0U",
        "outputId": "6702b58d-6202-4765-b180-90cd3ae9cd93"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(save_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "V23DNaPmxgZj",
        "outputId": "cb2f28c0-9e78-4a20-acb6-811d62d35823"
      },
      "outputs": [],
      "source": [
        "model = model.eval().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w1cfgrQxluw",
        "outputId": "37661781-737d-4cba-f122-58427645d160"
      },
      "outputs": [],
      "source": [
        "validate(model, test_loader, torch.nn.MSELoss())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ctOzZ3nCStNL"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_feature_nn(feature_nn, x_vals_compute, bias, x_vals_to_plot, ax=None, y_data=None, unscale_fun=None, q = 0):\n",
        "\n",
        "\n",
        "    with sns.axes_style('whitegrid'):\n",
        "      with sns.color_palette('dark'):\n",
        "\n",
        "        max_fun = lambda x: np.quantile(x, q = 1-q)\n",
        "        min_fun = lambda x: np.quantile(x, q = q)\n",
        "\n",
        "        valid_idx = (x_vals_compute >= min_fun(x_vals_compute)) & (x_vals_compute <= max_fun(x_vals_compute))\n",
        "        print((np.mean(valid_idx)))\n",
        "        print(len(x_vals_compute))\n",
        "        print(len(x_vals_to_plot))\n",
        "\n",
        "        x_vals_compute = x_vals_compute[valid_idx]\n",
        "        x_vals_to_plot = x_vals_to_plot[valid_idx]\n",
        "        y_data = y_data[valid_idx]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        min_compute = min(x_vals_compute)\n",
        "        max_compute = max(x_vals_compute)\n",
        "        range_compute = np.linspace(min_compute, max_compute, 1000)\n",
        "\n",
        "        min_plot = min(x_vals_to_plot)\n",
        "        max_plot = max(x_vals_to_plot)\n",
        "        range_plot = np.linspace(min_plot, max_plot, 1000)\n",
        "\n",
        "        feature_nn = feature_nn.eval()\n",
        "        with torch.no_grad():\n",
        "            model_input = torch.tensor(range_compute).to(device).reshape(-1, 1).float()\n",
        "            y_pred = feature_nn(model_input) + bias\n",
        "\n",
        "            if not unscale_fun is None:\n",
        "                y_pred = unscale_fun(y_pred)\n",
        "                y_data = unscale_fun(y_data)\n",
        "\n",
        "            if ax is None:\n",
        "                fig, ax = plt.subplots(figsize=(10, 10))\n",
        "            else:\n",
        "                fig = plt.gcf()\n",
        "\n",
        "            fig.set_size_inches(30/2, 10.5/2)\n",
        "            # Plot the model prediction\n",
        "            sns.lineplot(x=range_plot, y=y_pred.squeeze().cpu().numpy(), color=\"r\", label=\"Model Prediction\", linewidth=2)\n",
        "\n",
        "            # Plot the training data\n",
        "            if y_data is not None:\n",
        "                sns.scatterplot(x=x_vals_to_plot, y=y_data, color=\"blue\", alpha=0.3, s = 5, label=\"Training Data\", ax=ax)\n",
        "\n",
        "            # Customize the plot\n",
        "            ax.set_xlabel(\"X\", fontsize=15)\n",
        "            ax.set_ylabel(\"Y\", fontsize=15)\n",
        "            ax.spines[\"top\"].set_visible(False)\n",
        "            ax.spines[\"right\"].set_visible(False)\n",
        "            ax.spines[\"bottom\"].set_linewidth(1.5)\n",
        "            ax.spines[\"left\"].set_linewidth(1.5)\n",
        "            ax.tick_params(axis='both', which='major', labelsize=15, width=1.5)\n",
        "            #ax.set_xticks(fontsize=15)\n",
        "\n",
        "            # Add a title and legend\n",
        "            #ax.set_title(\"Feature Neural Network\", fontsize=20)\n",
        "            ax.legend(fontsize=20, frameon=True)\n",
        "\n",
        "            # Add a grid\n",
        "            ax.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            return fig, ax\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "0NB12-AnSwRs",
        "outputId": "f6ec2c54-8cb2-499f-e9fe-fe3ec30e69b2"
      },
      "outputs": [],
      "source": [
        "\n",
        "for idx, name in enumerate(col_names):\n",
        "  unscale_fun = unstandardize_and_exp_targets\n",
        "\n",
        "  fig, ax = plot_feature_nn(model.feat_nam.feature_nns[idx], features[:, idx], bias = model.feat_nam.bias.item(), x_vals_to_plot = train_df_unscaled[name], y_data = targets_standardized, unscale_fun = unscale_fun, q= 0)\n",
        "  ax.set_title(f\"{name}\")\n",
        "  ax.set_xlabel(f\"{name}\", fontsize=14)\n",
        "  ax.set_ylabel(\"Y\", fontsize=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCzzUYwT5Lyl",
        "outputId": "6ae4b9e4-e059-4661-dfe5-81715d8931a7"
      },
      "outputs": [],
      "source": [
        "col_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "SRTIqDKv4em7",
        "outputId": "da0097dc-54f9-47f1-90ee-bab96cfadca8"
      },
      "outputs": [],
      "source": [
        "col_name = \"review_scores_value\"\n",
        "plot_title = \"Value of Review Scores\"\n",
        "idx = col_names.index(col_name)\n",
        "\n",
        "unscale_fun = unstandardize_and_exp_targets\n",
        "\n",
        "fig, ax = plot_feature_nn(model.feat_nam.feature_nns[idx], features[:, idx], bias = model.feat_nam.bias.item(), x_vals_to_plot = train_df_unscaled[col_name], y_data = targets_standardized, unscale_fun = unscale_fun,q = 0.01)\n",
        "\n",
        "\n",
        "ax.set_title(f\"{plot_title}\", fontsize=20)\n",
        "ax.set_xlabel(f\"{plot_title}\", fontsize = 20)\n",
        "ax.set_ylabel(\"Price in ZAR\", fontsize = 20)\n",
        "fig.set_size_inches(10, 6.66)\n",
        "\n",
        "plt.savefig(f'save_path/{col_name}.png',  bbox_inches='tight', dpi = 1000)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYr2_sUX6wbJ"
      },
      "outputs": [],
      "source": [
        "plt.savefig(f'save_path/_{i_idx}_{j_idx}.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bznym2ydXshT"
      },
      "outputs": [],
      "source": [
        "lr = 1e-4\n",
        "#opt = torch.optim.AdamW(list(model.mlp.parameters()) + list(model.feat_nam.parameters()), lr = lr, weight_decay=1e-3)\n",
        "opt = torch.optim.AdamW(model.mlp.parameters(), lr = lr, weight_decay=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa-lLR2NXj63"
      },
      "outputs": [],
      "source": [
        "# fine-tune just MLP\n",
        "\n",
        "train_loop(model = model,\n",
        "           optimizer = opt,\n",
        "           loss_fun = torch.nn.MSELoss(),     #torch.nn.L1Loss4\n",
        "           trainset = train_loader,\n",
        "           valset = val_loader,\n",
        "           print_mod = 1,\n",
        "           device = device,\n",
        "           early_stopping = True,\n",
        "           n_epochs_early_stopping = 100,\n",
        "           save_path = save_path,\n",
        "           n_epochs = 10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI6BzAkAHyXQ"
      },
      "source": [
        "# Train MLP-head first and then NAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm7LrlosH911"
      },
      "outputs": [],
      "source": [
        "\n",
        "mlp = MLP(n_input_units = 512,\n",
        "    n_hidden_units = 500,\n",
        "    n_skip_layers = 4,\n",
        "    dropout_rate = 0.5)\n",
        "\n",
        "model_cnn = CNN(\n",
        "    pretrained_model = pretrained_encoder,\n",
        "    mlp = mlp\n",
        "\n",
        ")\n",
        "\n",
        "nam = NAM(n_features=features.shape[-1],\n",
        "                            shallow_units=20,\n",
        "                            hidden_units=(20, 20, 20),\n",
        "                            activation= torch.nn.ReLU(),\n",
        "                            dropout=0.3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqyRgY0yPXL_"
      },
      "outputs": [],
      "source": [
        "nam_plus_cnn = NAM_plus_CNN(pretrained_encoder, mlp, nam)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuArXln8f35y"
      },
      "source": [
        "### Train MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWp9iPcXIT8I"
      },
      "outputs": [],
      "source": [
        "lr = 1e-3\n",
        "opt = torch.optim.AdamW(model_cnn.mlp.parameters(), lr = lr, weight_decay=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ASmfhsyIWmy"
      },
      "outputs": [],
      "source": [
        "save_path_cnn = \"\"\n",
        "\n",
        "\n",
        "train_loop(model = model_cnn,\n",
        "           optimizer = opt,\n",
        "           loss_fun = torch.nn.MSELoss(),     #torch.nn.L1Loss(\n",
        "           trainset = train_loader,\n",
        "           valset = val_loader,\n",
        "           print_mod = 1,\n",
        "           device = device,\n",
        "           early_stopping = True,\n",
        "           n_epochs_early_stopping = 5,\n",
        "           save_path = save_path_cnn,\n",
        "           n_epochs = 10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jK1uGHEBVIDo"
      },
      "outputs": [],
      "source": [
        "model_cnn.load_state_dict(torch.load(save_path_cnn))\n",
        "model_cnn = model_cnn.eval().to(device)\n",
        "\n",
        "validate(model_cnn, test_loader, torch.nn.MSELoss())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYpcNd6IgklU"
      },
      "source": [
        "### Train CNAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ryx7CY2haPl5"
      },
      "outputs": [],
      "source": [
        "lr = 1e-3\n",
        "opt = torch.optim.AdamW(nam_plus_cnn.parameters(), lr = lr, weight_decay=1e-3)  # train only nam on top of fixed cnn  # THIS TRAINS ENCODER AS WELL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaGvm91pPjeG"
      },
      "outputs": [],
      "source": [
        "save_path_nam_plus_cnn = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXNrqjvCI5Xx"
      },
      "outputs": [],
      "source": [
        "save_path_nam_plus_cnn = \"\"\n",
        "\n",
        "nam_plus_cnn = nam_plus_cnn.to(device)\n",
        "train_loop(model = nam_plus_cnn,\n",
        "           optimizer = opt,\n",
        "           loss_fun = torch.nn.MSELoss(),     #torch.nn.L1Loss(\n",
        "           trainset = train_loader,\n",
        "           valset = val_loader,\n",
        "           print_mod = 1,\n",
        "           device = device,\n",
        "           early_stopping = True,\n",
        "           n_epochs_early_stopping = 100,\n",
        "           save_path = save_path_nam_plus_cnn,\n",
        "           n_epochs = 10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Zg0SPqPeMgh"
      },
      "outputs": [],
      "source": [
        "nam_plus_cnn.load_state_dict(torch.load(save_path_nam_plus_cnn))\n",
        "nam_plus_cnn = nam_plus_cnn.eval().to(device)\n",
        "\n",
        "validate(nam_plus_cnn, test_loader, torch.nn.MSELoss())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYQrQQE6PbJ8"
      },
      "source": [
        "# Inspect effect of interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yT0R4QznK3P"
      },
      "outputs": [],
      "source": [
        "path = \"\"\n",
        "\n",
        "data = ImageDataset(path, image_size=conf.img_size, exts=['jpg', 'JPG', 'png'], do_augment=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-IcwxIfE8vG"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "files = [f for f in listdir(path) if isfile(join(path, f))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc9KwCTkFFNQ"
      },
      "outputs": [],
      "source": [
        "fileNum2fileIdx = {int(name.replace(\".jpg\", \"\")): i for i, name in enumerate(files)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NqCM3DfFdjp"
      },
      "outputs": [],
      "source": [
        "i_idx = fileNum2fileIdx[761]\n",
        "j_idx = fileNum2fileIdx[587]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgkDlYvmm6mP"
      },
      "outputs": [],
      "source": [
        "i_idx = 5\n",
        "j_idx = 26\n",
        "batch = torch.stack([\n",
        "    data[i_idx]['img'],\n",
        "    data[j_idx]['img'],\n",
        "])\n",
        "\n",
        "# 860 and 595"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSXs4HIiQQ1_"
      },
      "outputs": [],
      "source": [
        "plt.imshow(batch[0].permute([1, 2, 0]) / 2 + 0.5)\n",
        "plt.show()\n",
        "plt.imshow(batch[1].permute([1, 2, 0]) / 2 + 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFoCb82wQ3Tv"
      },
      "outputs": [],
      "source": [
        "cond = pretrained_encoder.encode(batch.to(device))\n",
        "xT = pretrained_encoder.encode_stochastic(batch.to(device), cond, T=250)  #250 (original) # 10 (smooth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F8bKXDDRbtU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "n_interpolation_steps = 6\n",
        "alpha = torch.tensor(np.linspace(0, 1, n_interpolation_steps, dtype=np.float32)).to(cond.device)\n",
        "intp = cond[0][None] * (1 - alpha[:, None]) + cond[1][None] * alpha[:, None]\n",
        "\n",
        "def cos(a, b):\n",
        "    a = a.view(-1)\n",
        "    b = b.view(-1)\n",
        "    a = F.normalize(a, dim=0)\n",
        "    b = F.normalize(b, dim=0)\n",
        "    return (a * b).sum()\n",
        "\n",
        "theta = torch.arccos(cos(xT[0], xT[1]))\n",
        "x_shape = xT[0].shape\n",
        "intp_x = (torch.sin((1 - alpha[:, None]) * theta) * xT[0].flatten(0, 2)[None] + torch.sin(alpha[:, None] * theta) * xT[1].flatten(0, 2)[None]) / torch.sin(theta)\n",
        "intp_x = intp_x.view(-1, *x_shape)\n",
        "\n",
        "pred = pretrained_encoder.render(intp_x, intp, T=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xt5oqoA5Ut3"
      },
      "outputs": [],
      "source": [
        "n_interpolation_steps_pred = 100\n",
        "alpha_pred = torch.tensor(np.linspace(0, 1, n_interpolation_steps_pred, dtype=np.float32)).to(cond.device)\n",
        "intp_pred = cond[0][None] * (1 - alpha_pred[:, None]) + cond[1][None] * alpha_pred[:, None]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpYjmtZQRlsC"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, n_interpolation_steps, figsize=(5*10, 5))\n",
        "for i in range(len(alpha)):\n",
        "    ax[i].imshow(pred[i].permute(1, 2, 0).cpu())\n",
        "    ax[i].set_xlabel(\"Index: {}\".format(i))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoh-ttnmR0Ww"
      },
      "outputs": [],
      "source": [
        "plt.scatter(range(len(alpha_pred)), model.mlp(intp_pred).detach().cpu().numpy())\n",
        "plt.xticks(range(len(alpha_pred)))\n",
        "plt.title(\"Latent Interpolation\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evZLJNAY-1dW"
      },
      "outputs": [],
      "source": [
        "def find_nearest_multiples(x):\n",
        "    \"\"\"Find the nearest greater and smaller multiples of 1000 to a given float.\"\"\"\n",
        "\n",
        "    greater_multiple = (int(x)//500 + 1) * 500\n",
        "    smaller_multiple = (int(x)//500) * 500\n",
        "\n",
        "    return greater_multiple, smaller_multiple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKL0JgAQUXh4"
      },
      "outputs": [],
      "source": [
        "find_nearest_multiples(7777)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D829LMy16b3J"
      },
      "outputs": [],
      "source": [
        "n_interpolation_steps_pred = 100*len(alpha)\n",
        "alpha_pred = torch.tensor(np.linspace(0, 1, n_interpolation_steps_pred, dtype=np.float32)).to(cond.device)\n",
        "intp_pred = cond[0][None] * (1 - alpha_pred[:, None]) + cond[1][None] * alpha_pred[:, None]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsxH4pE67rSW"
      },
      "outputs": [],
      "source": [
        "list(range(10))[::3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vD7h84UBTk0w"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "with sns.axes_style('darkgrid'):\n",
        "    with sns.color_palette('dark'):\n",
        "      fig = plt.figure(figsize=(30, 10.5))\n",
        "      gs = fig.add_gridspec(2, n_interpolation_steps, height_ratios=[1, 1])\n",
        "\n",
        "      # Plot the predictions\n",
        "      preds_images = unstandardize_and_exp_targets(model.mlp(intp_pred).detach().cpu()).numpy()\n",
        "      ax3 = fig.add_subplot(gs[0, :])\n",
        "      ax3.plot(range(len(intp_pred)), preds_images, '-', markersize=20, c = \"black\",linewidth=3)\n",
        "\n",
        "\n",
        "\n",
        "      x_red_plot = np.linspace(0, n_interpolation_steps_pred, len(alpha))\n",
        "      img_red_plot = unstandardize_and_exp_targets(model.mlp(intp).detach().cpu()).numpy()\n",
        "      ax3.plot(x_red_plot, img_red_plot, 'o', markersize=15, c = \"black\")\n",
        "\n",
        "      y_min_r = find_nearest_multiples(min(preds_images)[0])[1]\n",
        "      y_max_r = find_nearest_multiples(max(preds_images)[0])[0]\n",
        "      #ax3.set_ylim([y_min_r, y_max_r])\n",
        "\n",
        "      #ax3.xaxis.set_ticks(range(len(pred)))\n",
        "      ax3.set_ylabel('Price in ZAR',  fontsize = 20)  # Set the y-axis label to 'ZAR'\n",
        "      #ax3.set_xlabel('Image')  # Set the y-axis label to 'ZAR'\n",
        "\n",
        "      # Set the y-axis tick labels to display values in ZAR\n",
        "      yticks = ax3.get_yticks()\n",
        "      yticklabels = ['{:,.0f}'.format(y) + ' ZAR' for y in yticks]\n",
        "      ax3.set_yticklabels(yticklabels,  fontsize = 20)\n",
        "\n",
        "      ax3.set_xticks([]) # remove x_ticks\n",
        "\n",
        "      # Plot the images\n",
        "      for i in range(len(alpha)):\n",
        "          axi = fig.add_subplot(gs[1, i])\n",
        "          axi.imshow(pred[i].permute(1, 2, 0).cpu())\n",
        "          axi.set_xticks([])\n",
        "          axi.set_yticks([])\n",
        "          #axi.set_xlabel(i + 1, fontsize = 20)\n",
        "\n",
        "      plt.tight_layout()\n",
        "\n",
        "      title = f\"Interpolation\"\n",
        "      ax3.set_title(title, fontsize = 30)\n",
        "\n",
        "      #plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
        "\n",
        "      # Increase the size of the scatter plot\n",
        "      #plt.setp(ax3, autoscale_on=True, xlim=(-0.5, len(pred) - 0.5), ylim=(preds_images.min() - 500, preds_images.max() + 500))\n",
        "      #plt.setp(ax3.yaxis.get_majorticklabels(), fontsize=10)\n",
        "      #plt.setp(ax3.xaxis.get_majorticklabels(), fontsize=20)\n",
        "\n",
        "      # Add x-tick labels to both subplots\n",
        "\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5ufgcXFXv3x"
      },
      "outputs": [],
      "source": [
        "#\n",
        "fig.savefig(f'save_path_{i_idx}_{j_idx}.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCmoo1BoOTTz"
      },
      "outputs": [],
      "source": [
        "idx_lis = [\n",
        "    (240, 468),\n",
        "    (75, 88),\n",
        "    (631, 644),\n",
        "    (966, 969),\n",
        "    (778, 784),\n",
        "    (849, 818),\n",
        "    (914, 934)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqPrnLNsPKld"
      },
      "outputs": [],
      "source": [
        "idx_lis2 = [\n",
        "    (837, 784),\n",
        "    (801, 799),\n",
        "    (749, 755),\n",
        "    (52, 417),\n",
        "    (343, 431),\n",
        "    (764, 768),\n",
        "    (88, 265),\n",
        "    (88, 298),\n",
        "    (88, 261),\n",
        "    (249, 261),\n",
        "    (295, 768),\n",
        "    (914, 934),\n",
        "    (914, 892),\n",
        "    (914, 802),\n",
        "    (914, 876),\n",
        "    (799, 801),\n",
        "    (876, 868)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5439ECJLYsO"
      },
      "outputs": [],
      "source": [
        "idx_lis3 = [\n",
        "    (274, 265),\n",
        "    (274, 261),\n",
        "    (274, 292),\n",
        "    (52, 62),\n",
        "    (168, 274),\n",
        "    (607, 755),\n",
        "    (811, 838),\n",
        "    (880, 914),\n",
        "    (29914, 971),\n",
        "    (29914, 976),\n",
        "    (29914, 969),\n",
        "    (29914, 2158),\n",
        "    (478, 498),\n",
        "    (478, 468),\n",
        "    (478, 423)\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3pIDW5_SEPe"
      },
      "outputs": [],
      "source": [
        "idx_lis4 = [\n",
        "    (417, 261),\n",
        "    (554, 602),\n",
        "    (431, 729),\n",
        "    (547, 706),\n",
        "    (795, 761),\n",
        "    (547, 815),\n",
        "    (818, 880),\n",
        "    (971, 969),\n",
        "    (991, 876),\n",
        "    (868, 876),\n",
        "    (795, 537)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-4Wu_pVJo7y"
      },
      "outputs": [],
      "source": [
        "idx_lis_raw = [\n",
        "    (0,4),\n",
        "    (3, 80),\n",
        "    (4, 53),\n",
        "    (5, 26),\n",
        "    (17, 10),\n",
        "    (27, 6),\n",
        "    (31, 32),\n",
        "    (54, 1),\n",
        "    (54, 56),\n",
        "    (55, 56),\n",
        "    (55, 68),\n",
        "    (61, 1),\n",
        "    (72, 73),\n",
        "    (74, 73),\n",
        "    (81, 78),\n",
        "    (83, 39),\n",
        "    (88, 69)\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_IYBfzUG6HI"
      },
      "outputs": [],
      "source": [
        "for i_idx, j_idx in idx_lis_raw:\n",
        "\n",
        "  batch = torch.stack([\n",
        "      data[i_idx]['img'],\n",
        "      data[j_idx]['img'],\n",
        "  ])\n",
        "\n",
        "  # 860 and 595\n",
        "\n",
        "  cond = pretrained_encoder.encode(batch.to(device))\n",
        "  xT = pretrained_encoder.encode_stochastic(batch.to(device), cond, T=250)  #250 (original) # 10 (smooth)\n",
        "\n",
        "  import numpy as np\n",
        "  n_interpolation_steps = 6\n",
        "  alpha = torch.tensor(np.linspace(0, 1, n_interpolation_steps, dtype=np.float32)).to(cond.device)\n",
        "  intp = cond[0][None] * (1 - alpha[:, None]) + cond[1][None] * alpha[:, None]\n",
        "\n",
        "  def cos(a, b):\n",
        "      a = a.view(-1)\n",
        "      b = b.view(-1)\n",
        "      a = F.normalize(a, dim=0)\n",
        "      b = F.normalize(b, dim=0)\n",
        "      return (a * b).sum()\n",
        "\n",
        "  theta = torch.arccos(cos(xT[0], xT[1]))\n",
        "  x_shape = xT[0].shape\n",
        "  intp_x = (torch.sin((1 - alpha[:, None]) * theta) * xT[0].flatten(0, 2)[None] + torch.sin(alpha[:, None] * theta) * xT[1].flatten(0, 2)[None]) / torch.sin(theta)\n",
        "  intp_x = intp_x.view(-1, *x_shape)\n",
        "\n",
        "  pred = pretrained_encoder.render(intp_x, intp, T=200)\n",
        "\n",
        "  n_interpolation_steps_pred = 100\n",
        "  alpha_pred = torch.tensor(np.linspace(0, 1, n_interpolation_steps_pred, dtype=np.float32)).to(cond.device)\n",
        "  intp_pred = cond[0][None] * (1 - alpha_pred[:, None]) + cond[1][None] * alpha_pred[:, None]\n",
        "\n",
        "  n_interpolation_steps_pred = 100*len(alpha)\n",
        "  alpha_pred = torch.tensor(np.linspace(0, 1, n_interpolation_steps_pred, dtype=np.float32)).to(cond.device)\n",
        "  intp_pred = cond[0][None] * (1 - alpha_pred[:, None]) + cond[1][None] * alpha_pred[:, None]\n",
        "\n",
        "  fig = plt.figure()\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "\n",
        "  with sns.axes_style('darkgrid'):\n",
        "      with sns.color_palette('dark'):\n",
        "        fig = plt.figure(figsize=(30, 10.5))\n",
        "        gs = fig.add_gridspec(2, n_interpolation_steps, height_ratios=[1, 1])\n",
        "\n",
        "        # Plot the predictions\n",
        "        preds_images = unstandardize_and_exp_targets(model.mlp(intp_pred).detach().cpu()).numpy()\n",
        "        ax3 = fig.add_subplot(gs[0, :])\n",
        "        ax3.plot(range(len(intp_pred)), preds_images, '-', markersize=20, c = \"blue\",linewidth=3)\n",
        "\n",
        "\n",
        "\n",
        "        x_red_plot = np.linspace(0, n_interpolation_steps_pred, len(alpha))\n",
        "        img_red_plot = unstandardize_and_exp_targets(model.mlp(intp).detach().cpu()).numpy()\n",
        "        ax3.plot(x_red_plot, img_red_plot, 'o', markersize=15, c = \"blue\")\n",
        "\n",
        "        y_min_r = find_nearest_multiples(min(preds_images)[0])[1]\n",
        "        y_max_r = find_nearest_multiples(max(preds_images)[0])[0]\n",
        "        #ax3.set_ylim([y_min_r, y_max_r])\n",
        "\n",
        "        #ax3.xaxis.set_ticks(range(len(pred)))\n",
        "        ax3.set_ylabel('Price in ZAR',  fontsize = 30)  # Set the y-axis label to 'ZAR'\n",
        "        #ax3.set_xlabel('Image')  # Set the y-axis label to 'ZAR'\n",
        "\n",
        "        # Set the y-axis tick labels to display values in ZAR\n",
        "        yticks = ax3.get_yticks()\n",
        "        yticklabels = ['{:,.0f}'.format(y) + ' ZAR' for y in yticks]\n",
        "        ax3.set_yticklabels(yticklabels,  fontsize = 30)\n",
        "\n",
        "        ax3.set_xticks([]) # remove x_ticks\n",
        "\n",
        "        # Plot the images\n",
        "        for i in range(len(alpha)):\n",
        "            axi = fig.add_subplot(gs[1, i])\n",
        "            axi.imshow(pred[i].permute(1, 2, 0).cpu())\n",
        "            axi.set_xticks([])\n",
        "            axi.set_yticks([])\n",
        "            #axi.set_xlabel(i + 1, fontsize = 20)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        title = f\"Interpolation\"\n",
        "        ax3.set_title(title, fontsize = 40)\n",
        "\n",
        "        #plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
        "\n",
        "        # Increase the size of the scatter plot\n",
        "        #plt.setp(ax3, autoscale_on=True, xlim=(-0.5, len(pred) - 0.5), ylim=(preds_images.min() - 500, preds_images.max() + 500))\n",
        "        #plt.setp(ax3.yaxis.get_majorticklabels(), fontsize=10)\n",
        "        #plt.setp(ax3.xaxis.get_majorticklabels(), fontsize=20)\n",
        "\n",
        "        # Add x-tick labels to both subplots\n",
        "\n",
        "\n",
        "        plt.savefig(f'save_path/interpolation_{i_idx}_{j_idx}.png', bbox_inches='tight')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bnhy5NqCCI4"
      },
      "source": [
        "## Image Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1YGevK4CE7n"
      },
      "outputs": [],
      "source": [
        "cls_conf = ffhq256_autoenc_cls()\n",
        "cls_conf.pretrain.path = \"\"\n",
        "cls_conf.latent_infer_path = \"\"\n",
        "cls_model = ClsModel(cls_conf)\n",
        "state = torch.load(\"\", map_location='cpu')\n",
        "print('latent step:', state['global_step'])\n",
        "cls_model.load_state_dict(state['state_dict'], strict=False);\n",
        "cls_model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL7sLkDZa_zu"
      },
      "outputs": [],
      "source": [
        "i_man_lis = [\n",
        "    88,\n",
        "    75,\n",
        "    249,\n",
        "    764,\n",
        "    784,\n",
        "    97,\n",
        "    880,\n",
        "    966,\n",
        "    761\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpXuY244b7e-"
      },
      "outputs": [],
      "source": [
        "CelebAttrDataset.id_to_cls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5n3PtNVcAfz"
      },
      "outputs": [],
      "source": [
        "attr_lis = [\n",
        " 'Attractive',\n",
        " 'Chubby',\n",
        " 'Double_Chin',\n",
        " 'Eyeglasses',\n",
        " 'Male',\n",
        " 'Young',\n",
        "'Pale_Skin',\n",
        "'Smiling',\n",
        "'Gray_Hair',\n",
        "'Big_Lips',\n",
        " 'Big_Nose',\n",
        " 'Black_Hair',\n",
        " 'Blond_Hair'\n",
        " ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5uyf9kAaxD9"
      },
      "outputs": [],
      "source": [
        "for i_m_idx in tqdm(i_man_lis):\n",
        "\n",
        "  i_idx = fileNum2fileIdx[i_m_idx]\n",
        "\n",
        "  img_idx = i_idx\n",
        "  batch = torch.stack([\n",
        "      data[img_idx]['img']\n",
        "  ])\n",
        "  plt.imshow(batch[0].permute([1, 2, 0]) / 2 + 0.5)\n",
        "  plt.show()\n",
        "\n",
        "  cond = pretrained_encoder.encode(batch.to(device))\n",
        "  xT = pretrained_encoder.encode_stochastic(batch.to(device), cond, T=50)  # 200\n",
        "\n",
        "  for a in attr_lis:\n",
        "\n",
        "    attribute_to_change = a\n",
        "    print(a)\n",
        "    for direct in [\"Plus\", \"Minus\"]:\n",
        "      direction = direct\n",
        "      sgn_dir = -1 if direction == \"Minus\" else 1\n",
        "      cls_id = CelebAttrDataset.cls_to_id[attribute_to_change]\n",
        "\n",
        "      cond2_org = cls_model.normalize(cond)\n",
        "      n_manipulation_steps = 10 # number of pictures to generate\n",
        "      step_range = sgn_dir*np.linspace(0, 0.4, num = n_manipulation_steps)\n",
        "\n",
        "      cond2_range = [cond2_org + step_size * math.sqrt(512) * F.normalize(cls_model.classifier.weight[cls_id][None, :], dim=1) for step_size in step_range]\n",
        "      cond2_lis = [cls_model.denormalize(cond_final) for cond_final in cond2_range]\n",
        "\n",
        "      intp_x = xT.repeat(len(intp), 1, 1, 1)\n",
        "      images = pretrained_encoder.render(intp_x, torch.cat(cond2_lis), T=200)\n",
        "\n",
        "\n",
        "      fig = plt.figure()\n",
        "\n",
        "      import matplotlib.pyplot as plt\n",
        "      import seaborn as sns\n",
        "\n",
        "      with sns.axes_style('darkgrid'):\n",
        "          with sns.color_palette('dark'):\n",
        "            fig = plt.figure(figsize=(30, 10.5))\n",
        "            gs = fig.add_gridspec(2, n_interpolation_steps, height_ratios=[1, 1])\n",
        "\n",
        "            # Plot the predictions\n",
        "            preds_images = unstandardize_and_exp_targets(mlp(pretrained_encoder.encode(images)).detach().cpu()).numpy()\n",
        "            ax3 = fig.add_subplot(gs[0, :])\n",
        "            ax3.plot(range(len(pred)), preds_images, '-o', markersize=15)\n",
        "\n",
        "            #ax3.xaxis.set_ticks(range(len(pred)))\n",
        "            ax3.set_ylabel('Price in ZAR')  # Set the y-axis label to 'ZAR'\n",
        "            #ax3.set_xlabel('Image')  # Set the y-axis label to 'ZAR'\n",
        "\n",
        "            # Set the y-axis tick labels to display values in ZAR\n",
        "            yticks = ax3.get_yticks()\n",
        "            yticklabels = ['{:,.0f}'.format(y) + ' ZAR' for y in yticks]\n",
        "            ax3.set_yticklabels(yticklabels)\n",
        "\n",
        "            ax3.set_xticks([]) # remove x_ticks\n",
        "\n",
        "            # Plot the images\n",
        "            for i in range(len(alpha)):\n",
        "                axi = fig.add_subplot(gs[1, i])\n",
        "                axi.imshow(images[i].permute(1, 2, 0).cpu())\n",
        "                axi.set_xticks([])\n",
        "                axi.set_yticks([])\n",
        "                axi.set_xlabel(i + 1, fontsize = 20)\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            title = f\"Manipulation of attribute \\\"{attribute_to_change}\\\" \"\n",
        "            ax3.set_title(title, fontsize = 20, pad=30)\n",
        "\n",
        "            #plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
        "\n",
        "            # Increase the size of the scatter plot\n",
        "            #plt.setp(ax3, autoscale_on=True, xlim=(-0.5, len(pred) - 0.5), ylim=(preds_images.min() - 500, preds_images.max() + 500))\n",
        "            #plt.setp(ax3.yaxis.get_majorticklabels(), fontsize=10)\n",
        "            #plt.setp(ax3.xaxis.get_majorticklabels(), fontsize=20)\n",
        "\n",
        "            # Add x-tick labels to both subplots\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "            fig.savefig(f'save_path/manipulation{i_idx}_{attribute_to_change}_direction_{direction}.png', bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti5LHddjCZZg"
      },
      "outputs": [],
      "source": [
        "i_idx = 54\n",
        "\n",
        "img_idx = i_idx\n",
        "batch = torch.stack([\n",
        "    data[img_idx]['img']\n",
        "])\n",
        "plt.imshow(batch[0].permute([1, 2, 0]) / 2 + 0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXjX5Fq9CSBM"
      },
      "outputs": [],
      "source": [
        "cond = pretrained_encoder.encode(batch.to(device))\n",
        "xT = pretrained_encoder.encode_stochastic(batch.to(device), cond, T=50)  # 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-XH_Y1aChWn"
      },
      "outputs": [],
      "source": [
        "print(CelebAttrDataset.id_to_cls)\n",
        "\n",
        "attribute_to_change = 'Eyeglasses'\n",
        "direction = \"Plus\"\n",
        "sgn_dir = -1 if direction == \"Minus\" else 1\n",
        "cls_id = CelebAttrDataset.cls_to_id[attribute_to_change]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hato4hyTCnZt"
      },
      "outputs": [],
      "source": [
        "cond2_org = cls_model.normalize(cond)\n",
        "n_manipulation_steps = 6 # number of pictures to generate\n",
        "step_range = sgn_dir*np.linspace(0, 0.4, num = n_manipulation_steps)\n",
        "\n",
        "cond2_range = [cond2_org + step_size * math.sqrt(512) * F.normalize(cls_model.classifier.weight[cls_id][None, :], dim=1) for step_size in step_range]\n",
        "cond2_lis = [cls_model.denormalize(cond_final) for cond_final in cond2_range]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XvC8Y5c3VTh"
      },
      "outputs": [],
      "source": [
        "intp_x = xT.repeat(len(intp), 1, 1, 1)\n",
        "images = pretrained_encoder.render(intp_x, torch.cat(cond2_lis), T=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwSf36OASaBp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXVvHYN0Lzfz"
      },
      "outputs": [],
      "source": [
        "fig1, ax1 = plt.subplots(1, n_manipulation_steps, figsize=(5*10, 5))\n",
        "for i in range(len(alpha)):\n",
        "    ax1[i].imshow(images[i].permute(1, 2, 0).cpu())\n",
        "    ax1[i].set_xlabel(\"Index: {}\".format(i))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PolCZLSdSaUL"
      },
      "outputs": [],
      "source": [
        "n_manipulation_steps_pred = 100*len(alpha) # number of pictures to generate\n",
        "step_range = sgn_dir*np.linspace(0, 0.4, num = n_manipulation_steps_pred)\n",
        "\n",
        "cond2_range = [cond2_org + step_size * math.sqrt(512) * F.normalize(cls_model.classifier.weight[cls_id][None, :], dim=1) for step_size in step_range]\n",
        "cond2_lis_pred = [cls_model.denormalize(cond_final) for cond_final in cond2_range]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKstk6QaC2Jy"
      },
      "outputs": [],
      "source": [
        "plt.scatter(range(n_manipulation_steps_pred), model.mlp(torch.stack(cond2_lis_pred).squeeze(1)).detach().cpu().numpy())\n",
        "plt.xticks(range(len(pred)))\n",
        "plt.title(\"Latent Interpolation\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj_ZL7YXTlU-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4M9ij_3TmVr"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "with sns.axes_style('darkgrid'):\n",
        "    with sns.color_palette('dark'):\n",
        "      fig = plt.figure(figsize=(30, 10.5))\n",
        "      gs = fig.add_gridspec(2, n_interpolation_steps, height_ratios=[1, 1])\n",
        "\n",
        "      # Plot the predictions\n",
        "      preds_images = unstandardize_and_exp_targets(model.mlp(torch.stack(cond2_lis_pred).squeeze(1)).detach().cpu()).numpy()\n",
        "      ax3 = fig.add_subplot(gs[0, :])\n",
        "      ax3.plot(range(len(intp_pred)), preds_images, '-', markersize=20, c = \"blue\",linewidth=3)\n",
        "\n",
        "\n",
        "\n",
        "      x_red_plot = np.linspace(0, n_manipulation_steps_pred, len(alpha))\n",
        "      img_red_plot = unstandardize_and_exp_targets(model.mlp(torch.stack(cond2_lis).squeeze(1)).detach().cpu()).numpy()\n",
        "      ax3.plot(x_red_plot, img_red_plot, 'o', markersize=15, c = \"blue\")\n",
        "\n",
        "      y_min_r = find_nearest_multiples(min(preds_images)[0])[1]\n",
        "      y_max_r = find_nearest_multiples(max(preds_images)[0])[0]\n",
        "      #ax3.set_ylim([y_min_r, y_max_r])\n",
        "\n",
        "      #ax3.xaxis.set_ticks(range(len(pred)))\n",
        "      ax3.set_ylabel('Price in ZAR',  fontsize = 30)  # Set the y-axis label to 'ZAR'\n",
        "      #ax3.set_xlabel('Image')  # Set the y-axis label to 'ZAR'\n",
        "\n",
        "      # Set the y-axis tick labels to display values in ZAR\n",
        "      yticks = ax3.get_yticks()\n",
        "      yticklabels = ['{:,.0f}'.format(y) + ' ZAR' for y in yticks]\n",
        "      ax3.set_yticklabels(yticklabels,  fontsize = 30)\n",
        "\n",
        "      ax3.set_xticks([]) # remove x_ticks\n",
        "\n",
        "      # Plot the images\n",
        "      for i in range(len(alpha)):\n",
        "          axi = fig.add_subplot(gs[1, i])\n",
        "          axi.imshow(images[i].permute(1, 2, 0).cpu())\n",
        "          axi.set_xticks([])\n",
        "          axi.set_yticks([])\n",
        "          #axi.set_xlabel(i + 1, fontsize = 20)\n",
        "\n",
        "      plt.tight_layout()\n",
        "\n",
        "      title = f\"Manipulation of Attribute \\\"{attribute_to_change}\\\" \"\n",
        "      ax3.set_title(title, fontsize = 40)\n",
        "\n",
        "      #plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
        "\n",
        "      # Increase the size of the scatter plot\n",
        "      #plt.setp(ax3, autoscale_on=True, xlim=(-0.5, len(pred) - 0.5), ylim=(preds_images.min() - 500, preds_images.max() + 500))\n",
        "      #plt.setp(ax3.yaxis.get_majorticklabels(), fontsize=10)\n",
        "      #plt.setp(ax3.xaxis.get_majorticklabels(), fontsize=20)\n",
        "\n",
        "      # Add x-tick labels to both subplots\n",
        "\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qw89f4XXFe1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuClass": "premium",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
